Although high accuracy tagging tools are generally available, sometimes their performance is not satisfactory and shall be increased further.
In cases where very high annotation quality is required, variance of tools should be considered.
Disparate methods can result in different sorts of errors, therefore their combination can yield better algorithms.
Even though this idea is not new, being present in both machine learning and \acrshort{nlp} literature, there has not been much work done on in this field for morphologically rich languages. 

In this section, the case of agglutinative languages is investigated by experimenting with Hungarian morphological disambiguator tools.
First, we give a brief overview about \acrshort{pos} tagger combination methods.
Next, discrepancies of two taggers are presented allowing us to create a new combination architecture.
Finally, we evaluate the proposed method by measuring its improvement over baseline tools used.

\subsection{Background}

For reviewing previous attempts on tagger combination approaches we rely on a recent study of Enríquez et al. \cite{Enriquez2013study}.
Their work not just investigates existing methods in detail, but also evaluates them for various  \acrshort{nlp} tasks (involving \acrshort{pos} tagging). 
Introducing the problem, the authors seek answers for the following question:
"What does it take for the combination to be successful?".
They conclude (referring to Hansen and Salamon \cite{Hansen1990}) that there are two fundamental requirements for the success. 
Classifiers employed must make different sorts of errors, and at the same time these algorithms should be more accurate than a random classifier.

Kuncheva \cite{Kuncheva2004} and Enríquez et al. \cite{Enriquez2013study} differentiate four levels of combination strategies. 
Firstly, the meta-classifier is a point of decision, since it greatly effects how categories suggested by the base classifiers are merged. 
Secondly, one can decide on algorithms used as base classifiers.
Thirdly, when feature vectors are utilized to represent examples, their variation can also impact the final result.
Finally, different datasets can be utilized to generate diverse taggers.

Regarding Enríquez et al. \cite{Enriquez2013study} and Xu et al. \cite{Xu1992}, combination algorithms can be also distinguished by their input.
Such methods can either receive only the \acrshort{pos} labels from the input taggers or it can get a more general output. 
A ranked list of labels -- where part-of-speech categories are sorted by their confidence level -- contains much more information. 
Furthermore, lists containing relevancy scores give the most background knowledge for combination methods.

Several ensemble strategies have been applied to \acrshort{pos} tagging in the literature, including voting, bagging, boosting, stacking (cf. \cite{Brill1998,Halteren2001,Kuba2004,Osterskov2009Italian,Pascal2012polish,Iliadis2012arabic}) or even using rules for aggregating outputs of input taggers \cite{Borin00somethingborrowed}.
Two of the most influential studies are the ones which presented by Brill and Wu \cite{Brill1998} and Halteren et al. \cite{Halteren2001}.
The former work presents the first attempts of combining English taggers.
The authors propose a memory-based meta-learning scheme which employs contextual and lexical clues.
In their experiments, the solution where the top-level learner always selects the output of one of the embedded taggers outperformed the more general scheme that allowed the output differ from either of the proposed tags.
The comprehensive study of Halteren et al. \cite{Halteren2001} compares several ensemble methods on three different corpora, showing that stacking methods can be used efficiently to train top-level classifiers for an optimal utilization of the corpus.
They found a scheme performing best characterized as generalized voting.
%Although it can yield annotation differing from the output of the embedded taggers, it can also be interpreted as a stacking method.
%However, the cardinality of the tag set and the dimensionality of the feature space were modest compared to that of our case.

A system of different architecture is presented by Hajič et al. \cite{Hajic2001}: in contrast to the parallel and hierarchical architecture of the systems above, it employs a serial combination of annotators starting with a rule-based morphological analyzer, followed by constraint-based filters feeding statistical taggers at the end of the chain. 


We extend the approach of and Brill and Wu \cite{Brill1998} and Halteren et al \cite{Halteren2001} by adapting their method to the morphological tagging of an agglutinative language. 
Our ensemble method builds on only the abstract output of input taggers without knowing their rank or score.
We use stacking with features adapted to Hungarian, furthermore, language-specific symbolic components are also utilized in our system.
To produce a full morphological tagger, the underlying architecture is modified to generate lemmata candidates as well.

\subsection{Discrepancies of taggers}

Evaluating taggers on general Hungarian can show that two of the best performing tools (our new method and HuLaPos) significantly diverge by the errors they made.
In our evaluation scheme, a detailed analysis of errors was carried out first, aiming to reveal their possible combined performance.
For this, we utilized the Humor-tagged Szeged Corpus (described in \ref{sec:porepos-gen}).
We kept the training sentences (80\% of the corpus), but the rest was split into two parts. The first half was employed for development purposes, while the second one was set apart for the final evaluation (cf. Table \ref{tab:comb-data}).

\begin{table}[H]
\centering
\caption{Number of sentences and tokens used for training, tuning and evaluating combination algorithms}\label{tab:comb-data}
\begin{tabular}{l r r}
\hline
& Tokens & Sentences \\
\hline
Training set & 980,225 & 56,792\\
Development set & 105,779 & 7,099 \\
Test set & 108,344 & 7,099 \\
\hline
\end{tabular}
\end{table}

First of all, we could not compare word class error rates one-by-one to reveal differences of taggers, since the cardinality of the tag-set is over 1,000.
Further on, we could neither rely on Brill's well-known formula (cf. \cite{Brill1998}), as it gives hard-to-interpret unlimited negative values when there is a considerable amount of overlap between the errors investigated.
Therefore, a new metric, called Own Error Rate ($\oer$), was introduced to measure the relatedness of the taggers' errors.
We used the formula 
\begin{equation}
\oer(A,B)=\frac{\text{\#errors of }A\text{ only}}{\text{\#errors of either }A\text{ or }B}
\end{equation}
for calculating the percentage of tagger $A$ being wrong but $B$ being correct in proportion of all errors made by either $A$ or $B$.

\begin{table}[H]
\centering
\caption{Error analysis of PurePos (PP) and HuLaPos (HLP) on the development set}\label{tab:comb-disambig-comp}
\begin{tabular}{l r r r}
\hline
& Tagging & Lemmatization & Full disambig. \\
\hline
% \hline
% Disagreement rate& 2.40\% & 1.98\% & 3.08\% \\
Agreement rate & 97.60\% & 98.02\% & 96.92\% \\
%Agr on not correct/One mistags & 22.09\% & 7.10\% & 18.26\% \\
They are right when they agree & 99.30\% & 99.85\% & 99.29\% \\
%%Agreement on erroneous annotation & 0.70\% & 0.15\% & 0.61\% \\
One is right when they disagree & 97.53\% & 98.89\% & 97.14\% \\
%\hline
$\oer($PP, HLP$)$ & 22.41\% & 11.66\% & 21.16\% \\
$\oer($HLP, PP$)$ & 53.58\% & 80.21\% & 58.24\% \\
\hline
\end{tabular}
\end{table}

To begin, we investigated the agreement of tools on the development set.
As Table \ref{tab:comb-disambig-comp} shows:
\begin{enumerate}
 \item they agree on the full annotation in most of the cases,
 \item matching tags and lemmata are almost always right, 
 \item one of them (frequently) knows the correct annotation even when their guesses do not match.
\end{enumerate}

Secondly, own error rates indicate that even though HuLaPos performs worse than PurePos, the errors are fairly balanced between them. 

\begin{table}[H]
\centering
\caption{Accuracy of the oracle and the baseline systems on the development set}\label{tab:comb-disambig-acc}
\begin{tabular}{l r r r}
\hline
& Tagging & Lemmatization & Full disambig. \\
\hline
PurePos & 98.57\% & 99.58\% & 98.43\% \\
HuLaPos & 97.61\% & 98.11\% & 97.03\% \\
Oracle & 99.26\% & 99.83\% & 99.22\% \\
\hline
\end{tabular}
\end{table}

Finally, the theoretical maximum performance of the combination (marked as oracle) is presented in Table \ref{tab:comb-disambig-acc}.
Assuming a hypothetical oracle always selecting the correct \emph{(tag, lemma)} pairs from the tools' suggestions, the accuracy of the better tagger can be further increased eliminating 72.73\% of PurePos' errors. 

\subsection{Improving PurePos with HuLaPos}

To utilize combination through cross-validation, the training set was split into 5 equal-sized parts.
Level-0 taggers (PurePos and HuLaPos) were trained 5 times using the 4/5 of the corpus while the rest of the sentences were annotated by both taggers in each round.
The union of these automatically annotated parts were used to train the (level-1) metalearners.
Furthermore, this technique allowed us to utilize all the training data in each level, yet separating the two phases of the training process. 

Concerning the question of choosing a level-1 learner we followed Witten et al. \cite{Witten2011} for investigating only ``relatively global, smooth'' algorithms.
We utilized \footnote{C4.5 decision tree algorithm was involved in our experiments, but it was unable to handle the large amount of feature data used.} the naïve Bayes (NB) classifier \cite{John1995} and instance-based (IB) learners \cite{Aha1991}.
The latter in addition to be simple, had been previously shown to perform well in similar combination tasks.
Another important decision was to apply metalearners only in cases of disagreement, since the tools’ agreement rate was high.

There are at least two parameters which must be set for IB learners.
First, a distance function needs to be selected, then the number of neighborhooding events has to be restricted.
In that way, we opted on using Manhattan distance and decided to rely only on the single closest item. 

Moving on, Hungarian has a tag-set with a cardinality of over a thousand and an almost unlimited vocabulary.
Therefore, we applied meta-algorithms choosing the tagger but not the tag. 

As regards features, we relied on the set proposed by Brill and Wu \cite{Brill1998}, since it had been shown (cf. \cite{Halteren2001}) to be simple but powerful. It (FS1 in Table \ref{tab:comb-feature-sets}) consists of several lexical properties such as
\begin{enumerate}
 \item the word to be tagged, 
 \item immediate neighbours of the token,
 \item tags suggested for the corresponding word,
 \item tags suggested for neighbouring tokens.
\end{enumerate}

\begin{table}[H]
\centering
\caption{Feature sets used in the combination experiments}\label{tab:comb-feature-sets}
\begin{tabular}{l l l}
\hline
Feature set & Base features & Additional features \\
\hline
FS1 & Brill-Wu & --- \\
FS2 & FS1 & whether the word contains a full stop or hyphen \\
FS3 & FS1 & use at most 5-character suffixes instead of the word form \\
FS4 & FS2, FS3 & --- \\ 
FS5 & FS1 & guessed tags for the second word both to the right and left \\
FS6 & FS4 & use at most 10-character suffixes instead of the word form \\
\hline
\end{tabular}
\end{table}

First, we examined how these attributes can be extended systematically (see Table \ref{tab:comb-feature-sets}) to fit languages with a productive morphology.
Since wordforms in Hungarian are composed of a lemma and numerous affixes, longer suffixes features are utilized to handle data sparseness issues.
Further on, wider context were also employed to manage the free word order nature of the language. 


Performing the experiments, we used the WEKA machine learning toolkit \cite{Hall2009}.
Improvements were measured on \acrshort{pos} tagging, lemmatization, as well as on the full annotation scenario.

\begin{table}[H]
\centering
\caption{Error rate reduction of combination algorithms on the development set. IB is the instance based learning algorithm, while NB denotes naïve Bayes.}\label{tab:comb-reduction-rates}
\begin{tabular}{l r r r r r r}
\hline
Task:& \multicolumn{2}{c}{Tagging} & \multicolumn{2}{c}{Lemmatization} & \multicolumn{2}{c}{Full annotation} \\
\hline
Feature set & \multicolumn{1}{l}{NB} & \multicolumn{1}{l}{IB} & \multicolumn{1}{l}{NB} & \multicolumn{1}{l}{IB} & \multicolumn{1}{l}{NB} & \multicolumn{1}{l}{IB} \\
\hline
FS1 & 19.03\% & 24.65\% & -6.21\% & 22.24\% & 5.06\% & 22.89\% \\
FS2 & 18.91\% & 24.82\% & -0.80\% & 23.85\% & 4.95\% & 23.16\% \\
FS3 & 21.04\% & 27.60\% & 0.80\% & 26.65\% & 18.42\% & 25.31\% \\
FS4 & 20.92\% & \underline{27.90\%} & 4.01\% & 26.65\% & 18.96\% & 25.20\% \\
FS5 & 16.37\% & 17.55\% & -19.24\% & 16.03\% & -0.70\% & 18.47\% \\
FS6 & 19.27\% & 27.30\% & -17.03\% & \underline{26.85\%} & 16.16\% & \underline{25.79\%} \\
\hline
\end{tabular}
\end{table}


Table \ref{tab:comb-reduction-rates} shows \acrlong{err} scores of different systems compared to PurePos. 
These results reveal that naïve Bayes classifier (NB) performs significantly worse than instance-based learners (IB) even when using seemingly independent features.
Further on, lemma combination turned out to be an insoluble task for that classifier.
Improvements show that word shape features (FS2) always help on tagging, while increased contexts (FS5) are not as powerful.
An interesting outcome of combining \acrshort{pos} taggers was that the word to be tagged was not necessary amongst the features (see FS4 and FS6 at Table \ref{tab:comb-reduction-rates}).
However, utilization of longer suffixes boosts the performance.
In addition, they are also beneficial in cases where lemmatization is part of the task. 

Now we turn on experiments yielding the best combination architecture for full morphological annotation.

\subsubsection{Combination of morphological taggers}

A simple combination structure is to treat annotations as atomic units letting the metalearner choose the output of one of the baselines (cf. Figure \ref{fig:comb1}).
Results on the development set suggest utilizing instance-based methods with the FS6 feature set for this architecture (cf. ``Full annotation'' column in Table \ref{tab:comb-reduction-rates}). 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.2]{MorphTagging/comb1.png} 
  \caption{Combining the output of two morphological taggers}
  \label{fig:comb1}
\end{figure}

\subsubsection{Combining PoS taggers only}

Another plausible scheme is to combine only the \acrshort{pos} tagger modules of the tools (see Figure \ref{fig:comb2}).
However, in doing so, one has to deal with lemmatization as well.
A straightforward solution for this to employ the lemmatizer of the better annotator tool (PurePos).
Following this, the best tag selection model could be constructed (cf.
Table \ref{tab:comb-reduction-rates}) using instance based learning with the FS4 feature set.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.2]{MorphTagging/comb2.png} 
  \caption{Combining the output of two PoS taggers and using also a lemmatizer}
  \label{fig:comb2}
\end{figure}

Although this algorithm allows us to create a better morpho-syntactic tagger compared to that of above, the gain in lemmatization remains much lower (6.81\%).
Consequently, the overall accuracy improvement measured in the development set (25.26\%) is inferior.

\subsubsection{Multiple metalearners}

Finally, the best results are produced using two level-1 learners: one of them chooses the better lemmatizer while the other selects the optimal \acrshort{pos} tagger (cf. Figure \ref{fig:comb3}).
In that  way, this architecture can incorporate the best lemma and tag candidates (as in Table \ref{tab:comb-reduction-rates}) yielding superior performance.
However, a drawback of this configuration is that it may result in incompatible tag-lemma pairs\footnote{A lemma and a tag for a word is incompatible if the \acrshort{ma} can analyze the word, but no analysis contains both the lemma and the morpho-syntactic label.}.
To overcome this problem, this combination scheme is enhanced with the Humor morphological analyzer.
This component is used to discover and fix incompatibilities.
With this enhancement, we achieved  32.42\% of improvement on the development set.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.2]{MorphTagging/comb3.png} 
  \caption{Combining the output of two PoS taggers and lemmatizers}
  \label{fig:comb3}
\end{figure}

\subsection{Evaluation}

%In order to validate our hypothesis, stating that the last configuration performs the best, improvements
%Combination schemes presented are evaluated on the unseen test set. %, confirming the results presented above.

\begin{table}[H]
\centering
\caption{Relative error rate reduction on the test set compared to PurePos}\label{tab:comb-eval}
\begin{tabular}{l r r r}
\hline
System & Tagging & Lemmatization & Full disamb. \\
\hline
\textit{Oracle} & 48.60\% & 59.42\% & 51.53\% \\
Disamb. combination & 23.23\% & 23.55\% & 26.86\% \\
Tagger combination & 22.76\% & 13.77\% & 23.81\% \\
Multiple metalearners & 25.07\% & 29.89\% & \underline{28.90\%} \\
\hline
\end{tabular}
\end{table}

All the presented combination schemes are evaluated on the unseen test set. %, confirming the results presented above.
Results show (cf. Table \ref{tab:comb-eval}) that the hybrid architecture using a morphological analyzer achieved the best performance.
While other schemes could also increase the performance of PurePos, it resulted in the highest accuracy (98.90\%) fixing 28.90\% of the baseline system's errors.
This improvement score also shows that our system could capture more then half of the cases which can be fixed by a hypothetical oracle combinator.
Further on, the proposed method also gives the highest error rate reduction in both \acrshort{pos} tagging and lemmatization.
Concerning statistical significance, both the improvements of the presented schemes and their differences are significant at p < 0.05 (using Wilcoxon test of paired samples).
These results show that our new combination architecture can be used in cases when very high disambiguation accuracy is crucial.
Finally, we also confirmed that PurePos and HuLaPos can complement each other resulting in an improved morphological tagger. 



 

 

 

 
