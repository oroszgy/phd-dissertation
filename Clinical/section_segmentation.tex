Error propagation in a text-processing chain is usually a notable problem, therefore accurate text segmentation methods are essential to parse texts properly.
Moreover, notes written by doctors are extremely noisy containing errors which inhibit the application of existing tools.

Even though tokenization and sentence segmentation methods perform well on general Hungarian, they have serious difficulties on clinical texts.
These originate in special properties of the text involving
\begin{enumerate} %[\itshape a\upshape)]
 \item typing errors (i.e. mistyped tokens, nonexistent strings of falsely concatenated words) and
 \item nonstandard usage of the language.
\end{enumerate}
While errors of the first type can generally be corrected with a rule-based tool, others need advanced methods. 

In this section, a hybrid approach to segmentation of noisy clinical records is presented. 
The method consists of two phases: first, tokens are partially segmented; then, sentence boundaries are identified.
We start with detailing the background of our research and introducing resources used.
Then, key elements of tokenization and \gls{sbd} algorithms are described. 
Finally, our system is systematically evaluated on a gold standard corpus showing its superior performance.

\subsection{Previous work on text segmentation}

Even though, numerous approaches deal with English medical texts, only few attempts have been made (cf. \cite{Siklosi2012,Siklosi2013,Siklosi2013b}) for Hungarian. 
While the latter ones pay almost no attention on segmenting texts, most of the studies for English do not detail such methods. 
%We review general tokenization and sentence boundary detection techniques first, then describes their application on the biomedical domain.

The task of text segmentation is often composed of several subtasks: normalization of noisy text (when necessary), tokenization, and sentence boundary detection.  
Although, these are generally performed one after another, there are approaches (e.g. \cite{zhu2007unified}), where tokenization and \acrshort{sbd} are treated as a unified tagging problem (such as in \cite{mikheev2000tagging}). 
Further on, handling of abbreviations is also often involved in the segmentation process, since their identification helps detecting sentence and token boundaries.

As regards tokenization, it is generally treated as a simple engineering problem\footnote{In the case of alphabetic writing systems.} cutting off punctuation marks from words. 
On the contrary, \acrshort{sbd} is a rather researched topic. 
As Read et al. summarize \cite{read2012sentence}, sentence segmentation approaches fall into three classes: 
\begin{enumerate}
 \item rule-based methods employing domain- or language-specific knowledge (such as abbreviations); 
 \item supervised machine learning approaches, which may not be robust amongst domains (being specialist on the training corpus); and
 \item unsupervised learning methods extracting their knowledge from raw unannotated data. 
\end{enumerate}

As regards \acrshort{ml} attempts, one of the first pioneers was Riley \cite{riley1989some} who employed decision-tree learners to classify full stops. 
He utilized mainly lexical features (such as word length or case) to compute the probability of a word being sentence-initial or sentence-final. 
Next, Palmer et al. presented \cite{palmer1997adaptive} the SATZ system, employing machine learning algorithms with surface and syntactic features. 
Since it can be easily adjusted by modifying features, this system has been successfully applied to several European languages. 
Further on, the maximum entropy learning approach was utilized first by Reynar and Ratnaparkhi \cite{reynar1997maximum} to the task. 
Their system classifies tokens containing `.', `?' or `!' characters relying on contextual features and abbreviation lists. 
Recently, a similar approach has been presented by Gillick \cite{gillick2009sentence} for English, using support vector machines and resulting in state-of-the-art performance.

Beside machine learning algorithms, rule-based methods are also commonly applied for the tasks. 
E.g. Mikheev presents \cite{mikheev2002periods} a small set of rules for detecting sentence boundaries (\acrshort{sb}) with a high accuracy. 
In another system presented of him \cite{mikheev2000tagging}, the latter method is integrated into a PoS-tagging framework enabling the classification of punctuation marks. 
In doing so, they can be labeled as sentence boundaries, abbreviations or both. 
Moving on, Kiss and Strunk have introduced \cite{kiss2006unsupervised} an unsupervised method for sentence boundary detection in 2009. 
Their tool, Punkt uses scaled log-likelihood ratio for deciding whether a \emph{(word, full stop)} pair is a collocation or not.

Although tokenization and \acrshort{sbd} tasks are well established fields of natural language processing, there are only a few attempts aiming medical texts. 
These sentence segmentation attempts fall into two classes: some develop rule-based systems (e.g. \cite{XuSDJWD10}), while most of the studies employ supervised machine learning algorithms (such as \cite{apostolova2009automatic,cho2002text,Savova2010,taira2001automatic,tomanek2007sentence}).
Latter approaches usually train \acrlong{maxent} or \acrshort{crf} learners, thus large handcrafted training corpora are essential. 

Training data used are either domain-specific or general. 
In practice, domain-specific knowledge yield better performance, however Tomanek et al.  \cite{tomanek2006reappraisal} argue on using only a general-purpose corpus. 
Their results indicate that the domain of the training corpus is not critical (at least for German).

As regards Hungarian, there are only two tools available. 
Huntoken \cite{Halacsy2004} is an open source system based on Mikheev’s system, while \texttt{magyarlanc} \cite{zsibrata2013magyarlanc} has an adapted version of MorphAdorner’s rule-based tokenizer \cite{kumar2009monk} and sentence splitter. 
Both of them employ general-purpose methods utilizing language- and domain-specific rules and dictionaries.

This study introduces new methods for segmenting noisy clinical texts resulting in an accurate and viable tool.
For this, special properties of the target domain is investigated by creating a manually segmented corpus. 
Then, a method is presented which combines high precision rules with unsupervised learning.

\subsection{Clinical text used}
\label{sec:clin_corpus}

A gold standard corpus of clinical texts was collected and manually corrected in order to investigate and evaluate segmentation approaches.
This process involved several steps. 
First, input texts were normalized, as they were highly erroneous.
We had to deal with the following types of errors\footnote{Text normalization steps were carried out employing regular expressions.}:
\begin{enumerate}
 \item doubly converted characters, such as `\&amp;gt;',
 \item typewriter problems (e.g. `1' and `0' is written as `l' and `o'),
 \item dates and date intervals being in various formats with or without necessary whitespaces (e.g. `2009.11.11', `06.01.08'),
 \item missing whitespaces between tokens usually introduced various types of errors, such as:
 \begin{enumerate}
  \item measurements were erroneously attached to quantities (e.g. `0.12mg'),
  \item lack of whitespace around punctuation marks (e.g. `töröközegek.Fundus:ép.'),
 \end{enumerate}
 \item various formulation of numerical expressions.
\end{enumerate}
 
To investigate possible pitfalls of the algorithm being developed, the gold standard data is split into two sets of equal sizes: a development and a test set containing 1,320 and 1,310 sentences respectively. 
The first part is used to identify typical problems and to develop the segmentation methods. 
On the contrary, the second one is used to verify results. 

First of all, the distribution of abbreviations, punctuation marks and capitalization is investigated in these texts to reveal possible pitfalls. 
Comparing our data with a corpus of general Hungarian (Szeged Corpus \cite{Csendes2004}) uncovers numerous discrepancies: 
\begin{enumerate}
 \item 2.68\% of tokens found in clinical corpus sample are abbreviations while the same ratio for general Hungarian is only 0.23\%; 
 \item sentences taken from the Szeged Corpus almost always end in a sentence final punctuation mark (98.96\%), while these are totally missing from clinical statements in 48.28\% of the cases; 
 \item sentence-initial capitalization is a general rule in Hungarian (99.58\% of the sentences are formulated properly in the Szeged Corpus), but its usage is not common in the case of clinicians (12.81\% of the sentences start with a word that is not capitalized); 
 \item the amount of numerical data is significant in medical records (13.50\% of sentences consist exclusively of measurement data and abbreviations), while text taken from the general domain rarely contains statements that are full of measurements. 
\end{enumerate}

This investigation implies that general-purpose tools may have difficulties with texts from this domain. 
Such methods usually builds on features such as word capitalization and presence of punctuation marks, however their usage is significantly differ in our case. 

\subsection{Evaluation metrics}
\label{sec:metric}

There is no metric commonly used to measure segmentation methods, therefore we examine existing ones.
On one hand, researchers specializing in machine learning approaches prefer to calculate precision, recall and $F$-measure. 
However, these scores are often used for measuring the correctness of sentence boundaries only.
On the other hand, studies on speech recognition prefer to compute NIST and Word Error Rate. 

Recently, Read et al. have reviewed \cite{read2012sentence} the state-of-the-art of text segmentation proposing a unified metric to compare different approaches. 
Their method allows to measure sentence boundaries at any position labeling characters as sentence-finals or non sentence-finals. 
In doing so, simple accuracy measures the performance. 

Our work builds on the results of Read et al. \cite{read2012sentence}. 
We adapt their approach to the full segmentation task considering noisy texts. 
We consider the corpus as a sequence of characters and empty strings and treat text segmentation as a single classification problem. 
In that way, all the entities (either characters or empty string between them) are labeled with one of the following tags: 
\begin{description}
 \item[$\langle$T$\rangle$] --  if the entity is a token boundary,
 \item[$\langle$S$\rangle$] -- if it is a sentence boundary,
 \item[$\langle$None$\rangle$] -- otherwise.
\end{description}
This classification scheme enables us to compute accuracy of the unified segmentation task. 
Moreover, it allows us to perform further measurements as well.

As it is important to measure each subsystem's correctness, precision and recall are computed for both word tokenization and \acrshort{sbd}. 
Further on, word segmentation is evaluated with the standard $F_1$ measure, while $F_{0.5}$ is computed for the sentence boundary detection task. 
The latter calculation makes precision more important than recall. % for sentences of noisy texts. 
We do this because an erroneously split sentence may cause information loss\label{sec:loss}, while statements might still be extracted from longer multi-sentence text. 

\subsection{Segmentation methods}
\label{sec:clinical_segmentation}

Our system is built up from several components (cf. Figure \ref{fig:clin-segment-arch}). 
First, a symbolic component (referred as the baseline) marks word and sentence boundaries\footnote{Rules and heuristics used are formulated investigating the development corpus.} seeking for full stops. 
Then, an unsupervised learning extends its output.
Finally, rules employing capitalization yields further sentence boundaries.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.2]{Clinical/clin_segm_arch.png} 
  \caption{The architecture of the proposed method}
  \label{fig:clin-segment-arch}
\end{figure}

\subsubsection{Rule-based word tokenization and sentence segmentation}

Our baseline method is composed of two parts. 
First, it tokenizes words (BWT) using regular expressions implemented in standard tokenizers. %then some of the sentence boundaries are classified.
However, this algorithm does not try disambiguate all words containing periods, since it would need the proper recognition of domain-specific abbreviations as well. %Therefore it only marks just a few boundaries

Further on, sentence segmentation (BSBD) is carried out considering information loss (as described in section \ref{sec:loss}.) 
In that way, the method minimizes the possibility of making false-positive errors by splitting sentences only if there is a high confidence of success. 
We found that such cases are, when:
\begin{enumerate} 
 \item a period or exclamation mark directly follows another punctuation mark token\footnote{Question marks are not considered as sentence-final punctuation marks, since they generally indicate a questionable finding in clinical texts.};
 \item a line starts with a full date, and is followed by other words (The last white-space character before the date is marked as SB.);
 \item a line begins with the name of an examination followed by a semicolon and a sequence of measurements.
\end{enumerate}

Realization of these simple observations yield 100\% precision and 73.38\% recall on tokenization considering the development set. 
The corresponding values for detecting ends of sentences are 98.48\% and 42.60\% respectively. 
Since less than half of the sentence boundaries are discovered, this method needs further improvements.
In addition, a deeper analysis unfolds that the tokenization module has difficulties only with sentence final periods. 
We found that these sorts of errors are effects of the conservative tokenization algorithm, which left several words with punctuation mark attached ambiguous.
%In that way, the algorithm is adjusted incorporating unsupervised learning.

%\subsubsection{Improvements on sentence boundary detection}\label{sec:improvements}

In order to enhance the baseline method we consider investigating common information sources. 
There are two indicators generally used for detecting sentence boundaries.
\begin{description}
 \item[Period:] when a punctuation mark ($\bullet$) is attached to a word, a sentence boundary is found for sure only if the token is not an abbreviation.
 \item[Capitalization:] if a word starts with a capital letter and it is neither part of a proper name nor of an acronym, it indicates the beginning of a sentence.
\end{description}

However, these are not directly applicable in our case. 
First of all, clinicians introduce new abbreviations frequently which are not part of the standard, therefore a proper list cannot be collected easily. 
Further on, Latin words and abbreviations are sometimes capitalized by mistake as well as several subclauses. 
In addition, numerous sentence boundaries lack both of these indicators (as shown in Section \ref{sec:clin_corpus}).

\subsubsection{Unsupervised sentence boundary classification}

Even though these features do not function regularly, they can still be utilized without a list of possible abbreviations. 
It is enough to find \emph{evidence} for the separateness of a word and the subsequent full stop to classify a position as a sentence boundary. 
This idea has been introduced by Kiss and Strunk \cite{kiss2006unsupervised} and now it is adapted for clinical texts.

Although log-likelihood ratio was first used to identify collocations \cite{dunning1993accurate}, it has been adapted for detecting sentence boundaries as well (cf. Punkt\cite{kiss2006unsupervised}). 
The basic idea is to handle abbreviations as collocations of words and periods. 
In practice, this is formulated via a null hypothesis \eqref{eq:h0} and an alternative one \eqref{eq:ha}. 

\begin{equation} \label{eq:h0}
H_0: P(\bullet|w) = p = P(\bullet|\neg w)
\end{equation}

\begin{equation} \label{eq:ha}
H_A: P(\bullet|w) = p_1 \neq p_2 = P(\bullet|\neg w) 
\end{equation}

\begin{equation} \label{eq:loglambda}
\log \lambda = -2 \log \frac{L(H_0)}{L(H_A)}
\end{equation}


Firstly, $H_0$ expresses the independence of a \emph{(word, $\bullet$)} pair, while $H_1$ formulates that their co-occurrence is not just by chance. 
$\log \lambda$ score \eqref{eq:loglambda} measures their ratio. 
Further on, its distribution is asymptotical to $\chi^2$, thus  it can be applied as a statistical test \cite{dunning1993accurate}. 
Kiss and Strunk found this pure algorithm performs poorly (in terms of precision) for identifying abbreviation, thus they introduced several scaling factors \cite{kiss2006unsupervised}. 
In doing so, their method looses the asymptotic relation to $\chi^2$ and became a simple filtering algorithm.

Utilizing their ideas, we improved the original method in numerous ways. 
First of all, the inverse score ($iscore=1/log\lambda$) is used as a base, since it helps to find candidates co-occurring only by chance. 
Moving on, we introduce further scaling factors reviewing that of Punkt and adapting them to match the characteristics of the target domain.

Several sorts of abbreviations occur with relative low frequencies. Therefore, counts and count ratios do not indicate properly alone whether a token and the period is related in a clinical record. 
In that way, the first modification of Punkt \cite{kiss2006unsupervised} cannot be directly applied in our case, thus ignored. 

Next, length of words ($len$) has been shown to indicate abbreviations well. 
This is even true for medical texts, since shorter tokens tend to be abbreviations, while longer ones do not. 
Therefore, we reformulate the original function to penalize short words and reward longer ones. 
Having a medical abbreviation list of almost 200 \label{sec:abbrev} elements\footnote{The list is gathered with an automatic algorithm on the development corpus using word shape properties and frequencies. The most frequent elements are manually verified and corrected.} 
we found that that more than 90\% of the abbreviations are shorter than three characters. 
This fact led us to formulate the scaling factor as in \eqref{eq:s_l}. 
In doing so, this enhancement can also decrease the score of a bad candidate, which distinguishes it from the original formula of Kiss and Strunk.

\begin{equation} \label{eq:s_l}
S_{length}(iscore)= iscore \cdot \exp{(len/3-1)}
\end{equation}

Recently, Humor \cite{Proszeky1994,Proszeky2005}  has been extended with the content of a medical dictionary \cite{Orosz2013}. 
What is more, the analyzer is able indicate whether analyses refers to abbreviations.
Therefore, its output is used to enhance the sentence segmentation algorithm.  

An indicator function (cf. Equation \ref{eq:sign}) utilizes its output denoting whether a word can be an abbreviation or not.
Since, the morphological lexicon used is a well-established resource, our applications can rely on it with high confidence.
In doing so, the factor formulated (cf. Equation \ref{eq:s_m}) uses larger weights compared to others. 
This method raises the score of a full word, decreases that of an abbreviation, while values of unknown words are left as they were.

\begin{equation}\label{eq:sign}
 indicator_{morph}(word) =
  \begin{cases}
   1  & \text{if $word$ has an analysis of a known full word} \\
   -1 & \text{if $word$ has an analysis of a known abbreviation} \\
   0  & \text{otherwise}
  \end{cases}
\end{equation}

\begin{equation} \label{eq:s_m}
S_{morph}(iscore)= iscore \cdot \exp{( indicator_{morph} \cdot len^2)}
\end{equation}

Besides, utilization of an additional attribute of tokens can also boost the segmentation process.
Hyphens are generally not present in abbreviations but rather occurs in full words. 
Relying on this observation, $iscore$ is adjusted \eqref{eq:s_h} with a further indicator function:
$indicator_{hyphen}$ outputs $1$ only if the word contains a hyphen. 

\begin{equation} \label{eq:s_h}
S_{hyphen}(iscore)= iscore \cdot \exp{(indicator_{hyphen} \cdot len)}
\end{equation}

\begin{equation} \label{eq:scaling}
S = S_{length} \circ S_{morph} \circ S_{hyphen}
\end{equation}

Scaled $\log \lambda$ (cf. $S(iscore)$ in Equation \ref{eq:scaling}) is calculated for all \emph{(word, $\bullet$)} pairs not followed by any other punctuation mark. 
If this value is found to be higher than a threshold, the period is regarded as a sentence boundary and it is detached.\footnote{Threshold value is empirically set to $1.5$.} 
Otherwise, the joint token is treated as an abbreviation.

To investigate the improvement of our method, it is pipelined with the BSBD module showing 77.14\% recall and 97.10\% precision on the development set. 
Accuracy values indicate significant improvements, however they show that many sentence boundaries are still not found.

\subsubsection{Rules on capitalization}

To further improve the method, capitalization of words are utilized as well. 
We develop a rule-based component to decide whether a capitalized words can start a sentence or not.
Good \acrshort{sb} candidates of such tokens are the ones not following a non sentence terminating\footnote{Sentence terminating punctuation marks are the period and the exclamation mark for this task.} punctuation, and are not part of a named entity. 
Therefore, sequences of capitalized words are considered to be named entities and omitted as a first step. 
Then, the rest of the candidates are processed with the morphological analyzer used.
We employ a simple heuristic for detecting sentence boundaries:
if a word does not have a proper noun analysis but is capitalized it is marked as the beginning of a sentence.  
Our results on the development set indicates that this component also enhances the BSBD: it increases recall to 65.46\% while keeps precision high (96.37\%). 

\subsection{Evaluation}

% Our hybrid algorithm has been developed using the development set, thus it is evaluated against the rest of the data. 
% %As the starting point of our comparison, we present the accuracy values of the preprocessed input text and the baseline tokenized one. 

\begin{table}[H]
\centering
\caption{Accuracy of the input text and the baseline segmented one}
\label{tab:base}
\begin{tabular}{ l  r } 
\hline
& Accuracy \\ 
\hline
Raw corpus  & 97.55\% \\
BSBD & 99.11\% \\
+ LLR & 99.72\% \\
+ CAP & 99.26\% \\
+ LLR + CAP & 99.74\% \\
\hline
\end{tabular}
\end{table}

Evaluation is presented for each components showing their accuracies in Table \ref{tab:base}.
First, our improvements are compared both to the baseline module and to the raw preprocessed corpus.
The unsupervised \acrshort{sbd} algorithm is marked with LLR\footnote{Referring to the term log-likelihood ratio.}, while the last component is indicated by CAP.
Results show high accuracies for the overall segmentation task.
However, the metric applied is not well balanced, since accuracy for the raw corpus is relatively high.

Moving on, overall improvements are investigated calculating error reduction rates (in Table \ref{tab:reduction}). 
Comparison is carried out measuring enhancements over the baseline method (BSBD). 

\begin{table}[H]
\centering
\caption{Error rate reduction over the baseline method}
\label{tab:reduction}
\begin{tabular}{ l  r } 
\hline
& Error rate reduction\\
\hline
LLR & 58.62\% \\
CAP & 9.25\% \\
LLR + CAP & 65.50\% \\
\hline
\end{tabular}
\end{table}

Considering sentence boundaries, a more detailed analysis is got by computing precision, recall and $F_{0.5}$ values (in Table \ref{tab:prec_rec}). 
Data shows that each component significantly increases the recall, while precision is just barely decreased. 
All in all, the combined hybrid algorithm\footnote{It is the composition of the BWT, BSBD, LLR and CAP components.} brings significant improvement over the well-established baseline.

\begin{table}[H]
\centering
\caption{Evaluation of the proposed sentence segmentation algorithm compared with the baseline}
\label{tab:prec_rec}
\begin{tabular}{ l r r  r  } 
\hline
& Precision & Recall & $F_{0.5}$ \\
\hline
Baseline & 96.57\% & 50.26\% & 81.54\%  \\
+ LLR & 95.19\% & 78.19\% & 91.22\% \\
+ CAP & 94.60\% & 71.56\% & 88.88\% \\
+ LLR + CAP & 93.28\% & 86.73\% & \underline{91.89\%} \\
\hline
\end{tabular}
\end{table}


While our approach focuses on the sentence segmentation task, we show that it improves word tokenization as well. 
Table \ref{tab:tok_eval} presents measurements on word segmentation indicating that our enhancements results in a higher recall, while they do not significantly decrease precision. \label{sec:eval}

%0.9974	0.7494	0.8558
%0.9854	0.9532	0.9690
%0.9974	0.7494	0.8558
%0.9854	0.9532	0.9690

\begin{table}[h]
\centering
\caption{Comparing tokenization performance of the new tool with the baseline one}
\label{tab:tok_eval}
\begin{tabular}{ l r r r} 
\hline
& Precision & Recall & $F_{1}$ \\
\hline
Baseline & 99.74\% & 74.94\% & 85.58\%  \\
Hybrid system & 98.54\% & 95.32\% & \underline{96.90\%} \\
\hline
\end{tabular}
\end{table}

Besides, the proposed method is compared with freely available tools as well.
There are only two applications for Hungarian text segmentation: \texttt{magyarlanc} and Huntoken.
The latter can be slightly adapted to a new domain by providing a set of abbreviations, thus two versions of it are evaluated. 
The first one employs a set of general Hungarian abbreviations (HTG), while the second one utilizes an extended dictionary\footnote{As described in section \ref{sec:abbrev}.} containing medical ones as well (HTM). 
Further on, Punkt \cite{kiss2006unsupervised} is involved in our comparison as well as the OpenNLP \cite{Baldridge2002} toolkit. 
The latter tool is a general framework of \acrlong{maxent} methods, hence it can be applied to detect sentence boundaries as it is presented in \cite{reynar1997maximum}.
(We used the general-purpose Szeged Corpus to train the \acrlong{maxent} learner.) 

\begin{table}[h]
\centering
\caption{Comparison of the proposed hybrid \acrshort{sbd} method with competing ones}
\label{tab:comparison}
\begin{tabular}{ l r r r} 
\hline
& Precision & Recall & $F_{0.5}$ \\
\hline
\texttt{magyarlanc} & 72.59\% & 77.68\% & 73.55\% \\
HTG & 44.73\% & 49.23\% & 45.56\% \\
HTM & 43.19\% & 42.09\% & 42.97\% \\
Punkt & 58.78\% & 45.66\% & 55.59\%  \\
OpenNLP & 52.10\% & 96.30\% & 57.37\% \\
Hybrid system & 93.28\% & 86.73\% & \underline{91.89\%} \\
\hline
\end{tabular}
\end{table}

Results in Table \ref{tab:comparison} show that general segmentation methods fail on Hungarian clinical notes in contrast to our new method. 
The hybrid approach presented bears with both high precision and recall, providing accurate sentence boundaries.
It has been found that the \acrshort{maxent} approach has high recall as well, but boundaries marked by it are false positives in almost half of the cases. 
Further on, rules of \texttt{magyarlanc} seem to be robust, but the overall low performance inhibits its application for clinical texts. 
Finally, other tools do provide not just low recalls, but their precision values are still around 50\% limiting their applicability. 

In sum, the presented segmentation method successfully deals with several sorts of imperfect sentence and word boundaries.
It performs better in terms of precision and recall than competing ones, achieving 92\% of $F_{0.5}$-score. 
Finally, our results indicates that the new hybrid algorithm is a proper tool for processing clinical Hungarian.


