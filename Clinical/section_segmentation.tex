
Error propagation in a text-processing chain is usually a notable problem, therefore accurate text segmentation methods are essential to process any sort of text properly.
However, notes written by doctors are extremely noisy containing errors which inhibit applications of existing tools.

Even though existing segmentation methods for Hungarian performing well on general domains, they have serious difficulties on clinical texts.
These originate in special properties of the text involving
\begin{inparaenum}[\itshape a\upshape)]
 \item typing errors (i.e. mistyped tokens, nonexistent strings of falsely concatenated words) and
 \item nonstandard usage of Hungarian.
\end{inparaenum}
While errors of the first type can generally be corrected with a rule-based tool, others need advanced methods. 

In this section, a hybrid approach to normalization and segmentation of Hungarian clinical records is presented. 
The method consists of three phases: first, a rule-based clean-up step is performed; then tokens are partially segmented; finally, sentence boundaries are determined. 
We start by detailing the tool's architecture. 
Then, key elements of the sentence boundary detection (SBD) algorithm are described. 
Finally, the system presented is evaluated against a gold standard corpus, and is compared to other tools available.

\subsection{Previous work on text segmentation}

Even though, numerous approaches deal with English noisy texts, only a few attempts have been made (cf. \cite{Siklosi2012,Siklosi2013,Siklosi2013b}) for Hungarian. In addition, the latter studies completely ignores the segmentation problem. What is more, most of the attempt attempts for clinical English also ignores the description of the tokenization/SBD algorithm applied. Therefore, we start with reviewing general segmentation techniques then continue on describing which of them are used successfully for the biomedical domain.

The task of text segmentation is often composed of several subtasks: normalization of noisy text (when necessary), segmentation of words, and sentence boundary detection.  
Although, these subtasks are generally performed one after another, there are approaches (e.g. \cite{zhu2007unified}), where text segmentation and normalization are treated as a unified tagging problem. Further on, handling of abbreviations is often involved during the process, since their identification helps detecting sentence and token boundaries.

As regards tokenization, it is generally treated as a simple engineering problem\footnote{In the case of alphabetic writing systems.} aiming to split punctuation marks from word forms. On the contrary, SBD is a rather researched topic. As Read et al. summarize \cite{read2012sentence}, sentence segmentation approaches fall into three classes: 
\begin{inparaenum}[\itshape 1\upshape)]
 \item rule-based methods employing domain- or language-specific knowledge (such as abbreviations); 
 \item supervised machine learning approaches, which may not be robust amongst domains (being specialist on the training corpus); 
 \item unsupervised learning methods, which extract their knowledge from raw unannotated data. 
\end{inparaenum}

As regards ML attempts, one of the first methods was presented by Riley \cite{riley1989some} applying decision-tree learners for disambiguating full stops. He utilized mainly lexical features, such as word length and case, and probabilities of a word being sentence-initial or sentence-final. 
Next, the SATZ system of Palmer et al. \cite{palmer1997adaptive} employs machine learning algorithms employing contextual and PoS features as well. 
Since it can be easily adapted adjusting the features, this system has been successfully applied \cite{palmer1997adaptive} to several European languages. 
Further on, the maximum entropy learning approach was also utilized by Reynar and Ratnaparkhi \cite{reynar1997maximum}. 
Their system classifies tokens containing `.', `?' or `!' by using contextual features and a prepared abbreviation list. A similar approach for English has been presented recently by
Recently, Gillick \cite{gillick2009sentence}. Their system uses support vector machines resulting in a state-of-the-art performance.

Beside machine learning, rule-based methods are commonly applied for the tasks. E.g. Mikheev presents \cite{mikheev2002periods} a small set of rules for detecting sentence boundaries (SB) with a high accuracy. 
In another system presented by him \cite{mikheev2000tagging}, the latter method is integrated into a PoS-tagging framework. This enhancement enables the classification of punctuation marks labeling them either as a sentence boundary, an abbreviation or both. 
Moving on, Kiss and Strunk have presented an unsupervised segmentation method recently. 
Their system, Punkt \cite{kiss2006unsupervised} uses scaled log-likelihood ratio for deciding whether a \emph{(word, period)} pair is a collocation or not.

Although tokenization and SBD tasks are well established fields in natural language processing, there are only a few attempts aiming medical texts. 
Sentence segmentation attempts for clinical texts fall into two classes: some develop rule-based systems (e.g. \cite{XuSDJWD10}), while most of the studies employ supervised machine learning algorithms (such as \cite{apostolova2009automatic,cho2002text,Savova2010,taira2001automatic,tomanek2007sentence}).
These approaches usually train maximum entropy or CRF learners, thus large handcrafted training corpora are essential. Training data used is either domain-specific or general. 
In practice, training material from a related domain yields better performance. However Tomanek et al. argue \cite{tomanek2006reappraisal} argue on using general newswire texts,
%TODO: newswire vajon igaz?
showing that the domain of the training corpus is not critical.

As regards Hungarian, there are two text segmentation tools available. Huntoken \cite{halacsy2004creating} is an open source tool based on Mikheev’s rule-based system, while \texttt{magyarlanc} \cite{zsibrata2013magyarlanc} has an adapted version of MorphAdorner’s rule-based tokenizer \cite{kumar2009monk} and sentence splitter. Both of them are general-purpose employing rules and dictionaries.


Since we found that existing tools cannot process medical texts properly, this section present a study on segmenting texts of noisy clinical notes. First, we investigate special properties of the target by creating a manually segmented corpus. Considering the target data we present a method which combines high precision rules with an unsupervised SBD method with well established recall.

\subsection{Clinical text used}

A gold standard corpus is collected and manually corrected enabling the investigation and evaluation of segmentation approaches.
This process involved several steps. First, input texts had to be normalized first, since the data collected is highly erroneous.
We had to deal with the following errors\footnote{Text normalization is performed using regular expressions.}:
\begin{enumerate}
 \item doubly converted characters, such as `\&amp;gt;',
 \item typewriter problems (e.g. `1' and `0' is written as `l' and `o'),
 \item dates and date intervals that were in various formats with or without necessary whitespaces (e.g. `2009.11.11', `06.01.08'),
 \item missing whitespaces between tokens that usually introduced various types of errors, such as:
 \begin{enumerate}
  \item measurements 
  were erroneously attached to quantities (e.g. `0.12mg'),
  \item lack of whitespace around punctuation marks (e.g. `töröközegek.Fundus:ép.'),
 \end{enumerate}
 \item various formulation of numerical expressions.
\end{enumerate}
 
In order to investigate possible pitfalls of the algorithm being developed, the gold standard data was split into two sets of equal sizes: a development and a test set containing 1320 and 1310 sentences respectively. 
The first part is used to identify typical problems in the corpus and to develop the segmentation methods. The second part is used to verify our results. 

The distribution of abbreviations, punctuation marks and capitalization in a certain text help to reveal the unique segmentation problems of that documents. Therefore a comparison of clinical texts and a corpus of general Hungarian (Szeged Corpus \cite{Csendes2004}) is carried out: 
\begin{enumerate}
 \item 2.68\% of tokens found in clinical corpus sample are abbreviations while the same ratio for general Hungarian is only 0.23\%; 
 \item sentences taken from the Szeged Corpus almost always end in a sentence final punctuation mark (98.96\%), while these are totally missing from clinical statements in 48.28\% of the cases; 
 \item sentence-initial capitalization is a general rule in Hungarian (99.58\% of the sentences are formulated properly in the Szeged Corpus), but its usage is not common in the case of clinicians (12.81\% of the sentences start with a word that is not capitalized); 
 \item the amount of numerical data is significant in medical records (13.50\% of sentences consist exclusively of measurement data and abbreviations), while text taken from the general domain rarely contains statements that are full of measurements. 
\end{enumerate}

\subsection{Evaluation metrics}

There are no unified metric being commonly used for text segmentation tasks.
Researchers specializing in machine learning approaches prefer to calculate precision, recall and $F$-measure.
Others, having a  background in speech recognition, prefer to compute NIST and Word Error Rate. 
Recently, Read et al. have reviewed \cite{read2012sentence} the current state-of-the-art in sentence boundary detection proposing a unified metric for comparing the performance of different approaches. 
Their method allows to measure sentence boundaries at any position, since characters are labeled as sentence-finals or non sentence-finals. In doing so, simple accuracy can be used as a metric. 

Our work rely on the metric introduced by Read et al. generalizing it for our task. The text is considered as a sequence of characters and empty strings. Segmentation is treated as a single classification problem, where all of the entities (either characters or empty string between them) are labeled with the following tags: 
\begin{description}
 \item[$\langle$T$\rangle$] --  if the entity is a token boundary,
 \item[$\langle$S$\rangle$] -- if it is a sentence boundary,
 \item[$\langle$None$\rangle$] -- if the entity is neither.
\end{description}
In doing so, this classification scheme enables us to calculate accuracy for the unified task. 
Further on, it is important to measure the subsystem's performance, thus precision and recall values are calculated for both word tokenization and SBD. 
Precision becomes more important than recall for segmenting sentences. It is because
an erroneously split sentence may cause information loss\label{sec:loss}, while statements might still be extracted from multi-sentence text. 
Consequently, $F_{0.5}$ is computer for SBD, while word tokenization is evaluated with the standard $F_1$ measure. 

\subsection{Segmentation Methods}

Our system is built up from several components. 
First, we introduce a baseline rule-based method which is used for marking word and sentence boundaries\footnote{Rules and heuristics used are formulated investigating the development corpus.}. 
Next, we detail its extensions yielding better segmentation algorithms. %TODO: esetleg egy ábra elfér még.

\subsubsection{Baseline word tokenization and sentence segmentation}

The baseline rule-based method is composed of two parts. First, it tokenizes words (BWT) by using regular expressions implemented in standard tokenizers. This module does not try disambiguate periods attached to the ends of words, since proper handling of such words would need to recognize abbreviations properly. 

Sentence segmentation (BSBD) in a subsequent component being performed in a way to avoid information loss (as described in section \ref{sec:loss}.) 
In doing so, the method minimizes the possibility of making false-positive errors by splitting sentences only if there is a high confidence of success. 
These cases are when:
\begin{enumerate} 
 \item a period or exclamation mark directly follows another punctuation mark token\footnote{Question marks are not considered as sentence-final punctuation marks, since they generally indicate a questionable finding in clinical texts.};
 \item a line starts with a full date, and is followed by other words (The last white-space character before the date is marked as SB.);
 \item a line begins with the name of an examination followed by a semicolon and a sequence of measurements.
\end{enumerate}

Implementing these simple observation yields 100\% precision and 73.38\% recall for the token segmentation task on the development set. The corresponding values for the sentence boundary detection are 98.48\% and 42.60\% respectively. 
Results on the development set indicate that less than half of the sentence boundaries are discovered, thus reveal the need of further improvement.
Further on, an analysis of errors also unfolds that the tokenization module has difficulties only with sentence final periods. These sort of errors are the effects of the conservative tokenization algorithm, since words with punctuation mark attached are left ambiguous.
Therefore, this investigation implies that an advanced sentence boundary detection algorithm is necessary for achieving higher recall scores.

\subsubsection{Improvements on sentence boundary detection}\label{sec:improvements}

To improve SBD results of the baseline method we investigate the applicability of common techniques. There are two sort of indicators generally used for detecting sentence boundaries: 
\begin{description}
 \item[period] when a period ($\bullet$) is attached to a word a sentence boundary is found for sure only if the token is not an abbreviation;
 \item[capitalization] if a word starts with a capital letter and it is neither part of a proper name nor of an acronym.
\end{description}

Unfortunately, these are not directly applicable in our case. First of all, medical abbreviations are too diverse: clinicians usually introduce new ones not being part of the standard. 
Further on, Latin words and abbreviations are sometimes capitalized by mistake. In addition, some subclauses begin with capitalized words as well. 
Finally, as shown in Section \ref{}, several sentence boundaries lack both of these indicators.

Even though these features are not proper indicators they can still be for used finding sentences. In addition, one does not need a full list of possible abbreviations neither. It is enough to find \emph{evidence} for the separateness of a word and the period attached to classify this position as a sentence boundary. 
This idea was introduced by Kiss and Strunk \cite{kiss2006unsupervised} and is being adapted in this study.

Scale-log likelihood ratio was originally used for identifying collocations by Dunning \cite{dunning1993accurate}, however it has been adapted for the sentence segmentation task in Punkt. The basic idea used is to handle abbreviations as collocations of words and periods. In practice, this is formulated via a null hypothesis \eqref{eq:h0} and an alternative one \eqref{eq:ha}. 

\begin{equation} \label{eq:h0}
H_0: P(\bullet|w) = p = P(\bullet|\neg w)
\end{equation}
\vspace{-0.5cm}
\begin{equation} \label{eq:ha}
H_A: P(\bullet|w) = p_1 \neq p_2 = P(\bullet|\neg w) 
\end{equation}
\vspace{-0.5cm}
\begin{equation} \label{eq:loglambda}
log \lambda = -2 log \frac{L(H_0)}{L(H_A)}
\end{equation}


\eqref{eq:h0} expresses the independence of a \emph{(word, $\bullet$)} pair, while \eqref{eq:ha} formulates that their co-occurrence is not just by chance. $log \lambda$ score  \eqref{eq:loglambda} computes their ratio in a way to be asymptotically $\chi^2$ distributed. 
Therefore, it can be applied as a statistical test \cite{dunning1993accurate}. 
Kiss and Strunk found that pure $log \lambda$ score performs poorly\footnote{In terms of precision.} in abbreviation detection scenarios, thus they introduced scaling factors \cite{kiss2006unsupervised}. 
In doing so, their method loses the asymptotic relation to $\chi^2$ and becomes a simple filtering algorithm.

Utilizing their ideas we improved the original method in numerous places. 
First of all, the inverse of $\log\lambda$:$iscore=1/log\lambda$ is calculated as a base, since our goal is to find candidates co-occurring only by chance. In addition, we adapt existing scaling factors and introduce new ones to match the characteristics of the data.

The first scaling factor found in the original work \cite{kiss2006unsupervised} cannot be directly applied, since counts and count ratios do not indicate properly whether a token and the period is related in a clinical record. The reason behind this is that several sort of abbreviations with relative low frequencies. 
Next, length of words ($len$) has been shown to be a good indicator of abbreviations, since shorter tokens tend to be abbreviations, while longer ones do not. Thus, we reformulate the original function to penalize short words and reward longer ones. 
Having a medical abbreviation list of almost 200 \label{sec:abbrev} elements\footnote{The list is gathered with an automatic algorithm on the development corpus using word shape properties and frequencies. The most frequent elements are manually verified and corrected.} 
we found that that more than 90\% of the abbreviations are shorter than three characters. This fact led us to formulate the scaling factor in equation \eqref{eq:s_l}. 
In doing so, this modification can also decrease a score of candidate in contrast with the original formula in \cite{}.


\begin{equation} \label{eq:s_l}
S_{length}(iscore)= iscore \cdot \exp{(len/3-1)}
\end{equation}

Recently, HuMor \cite{Proszeky1994,Proszeky2005}  has been extended with the content of a medical dictionary \cite{Orosz:mszny2013}, thus this tool is used to enhance the sentence segmentation algorithm.  
Since the analyzer is able indicate whether an analyses refers to an abbreviation, its output is utilized by an indicator function \eqref{eq:sign}.
Furthermore, morphological lexicons are usually well-established resources, therefore applications can rely on them without any doubt. Consequently, a more confident factor is formulated (Equation \eqref{eq:s_m}) using larger weights compared to others. In doing so, the score is raised in case of full words, it is decreased for abbreviations, while values of unknown words are left as they were.

\begin{equation}\label{eq:sign}
 indicator_{morph}(word) =
  \begin{cases}
   1  & \text{if $word$ has an analysis of a known full word} \\
   -1 & \text{if $word$ has an analysis of a known abbreviation} \\
   0  & \text{otherwise}
  \end{cases}
\end{equation}

\begin{equation} \label{eq:s_m}
S_{morph}(iscore)= iscore \cdot \exp{( indicator_{morph} \cdot len^2)}
\end{equation}

Besides, another indicator has been found to fit well to the development data. Since, hyphens are generally not present in abbreviations but rather occurs in full words, the overall score is modified involving this observation. Thus, Equation \eqref{eq:s_h} is used raise the score of tokens having hyphens. For this $indicator_{hyphen}$ is utilized outputting $1$ only if the word contains a hyphen. 


\begin{equation} \label{eq:s_h}
S_{hyphen}(iscore)= iscore \cdot \exp{(indicator_{hyphen} \cdot len)}
\end{equation}

\begin{equation}
S = S_{length} \circ S_{morph} \circ S_{hyphen}
\end{equation}

Scaled $S(iscore)$ is calculated for all \emph{(word, $\bullet$)} pairs not followed by another punctuation mark. If this value is higher than a threshold, the period is regarded as a sentence boundary and it is detached.\footnote{Threshold value is empirically set to $1.5$.}
Investigating the scaled $\log \lambda$ performance, it is pipelined after the BSBD module resulting in 77.14\% recall and 97.10\% precision on the development set. These values indicates significant improvement but shows that many sentence boundaries are still not found.

To further improve the method word capitalization is utilized as well. A subsequent rule-based component is created dealing with capitalized words. 
Good SB candidates of these are the ones not following a non sentence terminating\footnote{Sentence terminating punctuation marks are the period and the exclamation mark for this task.} punctuation, and are not part of a named entity. 
Therefore, sequences of capitalized words are considered to be named entities and omitted as a first step. Then the rest of the candidates are processed with the morphological analyzer employing a simple heuristic for detecting sentence boundaries. If a word does not have a proper noun analysis but is capitalized it is marked as the beginning of a sentence.  
Investigating the component enhancement over BSBD on the development set we found that this module results in an increased recall (65.46\%) while keeps precision high (96.37\%). 

\subsection{Evaluation}

% Our hybrid algorithm has been developed using the development set, thus it is evaluated against the rest of the data. 
% %As the starting point of our comparison, we present the accuracy values of the preprocessed input text and the baseline tokenized one. 

\begin{table}[h]
\centering
\caption{Accuracy of the input text and the baseline segmented one}
\label{tab:base}
\begin{tabular}{ l  r } 
\hline
& Accuracy \\ 
\hline
Raw corpus  & 97.55\% \\
BSBD & 99.11\% \\
+ LLR & 99.72\% \\
+ CAP & 99.26\% \\
+ LLR + CAP & 99.74\% \\
\hline
\end{tabular}
\end{table}

Accuracy values in Table \ref{tab:base} measures the tool's performance on the overall segmentation task. 
All of the components are evaluated and compared to the baseline module and the raw preprocessed corpus.
The unsupervised SBD algorithm is marked with \emph{LLR}\footnote{Referring to the term log-likelihood ratio.}, while the second component is indicated by the term \emph{CAP}.
This metric is not a well balanced, since its values are relatively high even for the preprocessed text. Therefore we present individual improvement scores as well. 
Relative error rate reduction scores are provided in Table \ref{tab:reduction}. These values are calculated over the baseline method (BSBD) for each component and their collaboration as well. 

\begin{table}[h]
\centering
\caption{Error rate reduction over the accuracy of the baseline method}
\label{tab:reduction}
\begin{tabular}{ l  r } 
\hline
& Error rate reduction\\
\hline
LLR & 58.62\% \\
CAP & 9.25\% \\
LLR + CAP & 65.50\% \\
\hline
\end{tabular}
\end{table}


Considering sentence boundaries, a more detailed analysis is got by computing precision, recall and $F_{0.5}$ values in Table \ref{tab:prec_rec}. These results show that each component significantly increases the recall, while precision is just barely decreased. All in all, the combined hybrid algorithm\footnote{It is the composition of the BWT, BSBD, LLR and CAP components.} brings significant improvement over the well-established baseline.

\begin{table}[h]
\centering
\caption{Evaluation of the proposed sentence segmentation algorithm compared with the baseline}
\label{tab:prec_rec}
\begin{tabular}{ l r r  r  } 
\hline
& Precision & Recall & $F_{0.5}$ \\
\hline
Baseline & 96.57\% & 50.26\% & 81.54\%  \\
+ LLR & 95.19\% & 78.19\% & 91.22\% \\
+ CAP & 94.60\% & 71.56\% & 88.88\% \\
+ LLR + CAP & 93.28\% & 86.73\% & \underline{91.89\%} \\
\hline
\end{tabular}
\end{table}


While our approach has a focus on the sentence segmentation task, we show that it improves word tokenization as well. Table \ref{tab:tok_eval} presents measurements on word tokenization showing that our enhancements results in a higher recall, while they does not significantly decrease precision. \label{sec:eval}

\begin{table}[h]
\centering
\caption{Comparing tokenization performance of the new tool with the baseline one}
\label{tab:tok_eval}
\begin{tabular}{ l r r r} 
\hline
& Precision & Recall & $F_{1}$ \\
\hline
Baseline & 99.74\% & 74.94\% & 85.58\%  \\
Hybrid system & 98.54\% & 95.32\% & \underline{96.90\%} \\
\hline
\end{tabular}
\end{table}

Besides, we compare our method with freely available tools as well.
There are only two application aiming Hungarian text segmentation: are \texttt{magyarlanc} Huntoken.
The latter can be slightly adapted to a new domain by providing a set of abbreviations, thus two versions of it are evaluated. 
The first employs a set of general Hungarian abbreviations (\emph{HTG}), while the second utilizes an extended dictionary\footnote{As described in section \ref{sec:abbrev}.} containing medical ones as well(\emph{HTM}). 
Further on, Punkt \cite{kiss2006unsupervised} is involved in our comparision as well as the OpenNLP \cite{Baldridge2002} toolkit. The latter tool is a general framework building on maximum entropy methods, thus it can be applied to detect sentence boundaries as it is presented in \cite{reynar1997maximum}. For this we used the general-purpose Szeged Corpus as a training material. 

\begin{table}[h]
\centering
\caption{Comparision of the proposed hybrid SBD method with competing ones}
\label{tab:comparison}
\begin{tabular}{ l r r r} 
\hline
& Precision & Recall & $F_{0.5}$ \\
\hline
magyarlanc & 72.59\% & 77.68\% & 73.55\% \\
HTG & 44.73\% & 49.23\% & 45.56\% \\
HTM & 43.19\% & 42.09\% & 42.97\% \\
Punkt & 58.78\% & 45.66\% & 55.59\%  \\
OpenNLP & 52.10\% & 96.30\% & 57.37\% \\
Hybrid system & 93.28\% & 86.73\% & \underline{91.89\%} \\
\hline
\end{tabular}
\end{table}

First, results in Table \ref{tab:comparison} indicates that general segmentation methods fail on Hungarian clinical records in contrast to our new method. The hybrid approach presented bears with high precision and recall providing accurate sentence boundaries.
It has been found that the maxent approach has high recall as well, but boundaries marked by it are false positives in almost half of the cases. 
Further on, rules of \texttt{magyarlanc} seem to be robust, but the overall low performance inhibits its application for clinical texts. 
Finally, other tools do not just provide low recalls, but their precision values are around 50\% being too low for practical purposes. 

All in all, the presented segmentation method successfully deals with several sorts of imperfect sentence and word boundaries.
It performs better in terms of precision and recall than competing ones achieving a 92\% of $F_{0.5}$-score. Our results indicates that the the new hybrid algorithm is a proper tool for processing clinical Hungarian.


