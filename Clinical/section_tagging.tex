Beside text segmentation, morphological tagging is also an indispensable task for information extraction scenarios. 
Even though tagging of general texts is well-known and considered to be solved, medical texts pose new challenges to researchers. 
In addition, English has been the main target of many studies investigating the biomedical domain up to the present time. 
Furthermore, there are just a few approaches for non- English data, neglecting agglutinative languages and particularly Hungarian.

This section investigates the tagging of clinical Hungarian by adapting existing methods. % which results in an accurate tagger.
Our work is structured as follows. 
Related studies are described first, then a corpus of clinical notes is presented.
%Latter texts has been collected for developing and evaluating tagging algorithms. 
%In Section \ref{sec:baseline}, we introduce a baseline morphological disambiguation chain and a detailed error analysis. 
Finally, domain adaptation enhancements are introduced which are then evaluated on the test corpus.

\subsection{Background}
\label{sec:biomed_tag}

In general, tagging of biomedical texts has an extensive literature, since numerous resources are accessible for English. 
On the contrary, much less manually annotated corpora of clinical texts are available. 
Further on, most of the work in this field was done for English, while only a few attempts were published for morphologically rich languages (e.g. \cite{oleynik2009performance,rost2008lessons}).

First of all, a common approach for tagging biomedical text is to train supervised sequence-classifiers. 
However, a drawback of these methods is that they require manually annotated texts which are hard to create. %labor-intense human work. 
Considering the types of training material, domain-specific corpora are used either alone \cite{pakhomov2006developing,Savova2010,Smith2006} or in conjunction with a (sub)corpus of general English \cite{coden2005domain,ferraro2013improving,miller2007building}. % as training data. 
While utilizing texts only from the target domain yields acceptable performance \cite{pakhomov2006developing,Savova2010,Smith2006}, 
several experiments have shown that accuracy further increases with incorporating annotated sentences from the general domain as well \cite{barrett2011token,coden2005domain}. 
It is shown (e.g. \cite{pestian2004development}) that the more data is used from the reference domain, the higher accuracy can be achieved. 
However, Hahn and Wermter argue for training learners only on general corpora \cite{hahn2004tagging} (for German). 
Besides, there are studies on automatic selection of the training data (e. g. \cite{liu2007heuristic}). % to increase accuracy. 
What is more, there are algorithms (such as \cite{choi2012fast}) learning from several domains parallelly thus delaying the model selection decision to the decoding process. 

Next, utilization of domain-specific lexicons is another way of adapting taggers, as they can improve tagging performance significantly \cite{coden2005domain,ruch2000minimal}. 
Some studies extend existing \acrshort{pos} dictionaries \cite{divita2006dtagger}, while others build new ones \cite{Smith2006}. 
In brief, all such experiments yield significantly reduced error rates. 

Concerning tagging algorithms, researchers tend to prefer already existing applications. 
One of the most popular system is the OpenNLP toolkit \cite{Baldridge2002}, which is e.g. the basis of the cTakes system \cite{Savova2010}.
Further on, Brill’s method \cite{Brill1992} and TnT \cite{Brants2000} are widely used (e.g. \cite{hahn2004tagging,Savova2010,pestian2004development}) as well. 
Besides, other \acrshort{hmm}-based solutions were also shown to perform well \cite{barrett2011token,coden2005domain,divita2006dtagger,hahn2004tagging,pakhomov2006developing,rost2008lessons,ruch2000minimal} on biomedical texts. 

Moving on, a number of experiments have revealed \cite{ferraro2013improving,ruch2000minimal,Smith2006} that domain-specific \acrshort{oov} words are behind the reduced performance of taggers. 
Therefore, successful methods employ either guessing algorithms \cite{barrett2011token,divita2006dtagger,rost2008lessons,ruch2000minimal,Smith2006} or broad-coverage lexicons (as detailed above). 
Beyond supervised algorithms, other approaches were also shown to be effective: Miller et al. \cite{miller2007building} use semi-supervised methods;
%\footnote{This algorithm needs raw data from the target domain, while an annotated general corpus is still used.}; 
Dwinedi and Sukhadeve build a tagger based only on rules \cite{dwivedi8rule}; while Ruch et al. propose a hybrid system \cite{ruch2000minimal}. 
Further on, automatic domain adaptation methods (such as EasyAdapt \cite{daume2007frustratingly}, ClinAdapt \cite{ferraro2013improving} 
or reference distribution modelling  \cite{tateisi2006subdomain}) also perform well. As a drawback, they need an appropriate amount of manually annotated data from the target domain limiting their applicability. 

%First, we examine special properties of clinical notes, then a proper disambiguation methodology is being presented. 
Our method builds on a baseline tagging chain composed of a trigram tagger (introduced in Section \ref{sec:purepos}) and a broad coverage morphological analyzer. 
The latter tool employs a domain-adapted lexicon, while the tagger is adapted to the domain with further components.
%For this, an error analysis of the baseline system is used to adjust the output of the tagger to the domain.

\subsection{The clinical corpus}

As there is no corpus of clinical records available manually annotated with morphological analyses, a new one was created. 
These texts contain about 600 sentences extracted from notes of 24 different clinics. 
First, textual parts of the records were identified (as described in \cite{Siklosi2012}), then the paragraphs to be processed were selected randomly. 
After these, sentence boundary segmentation, tokenization and normalization was performed manually aided by methods of Section \ref{sec:clinical_segmentation}. 
Manual spelling correction was carried out using the system of Siklósi et al. \cite{Siklosi2013}. 
Finally, morphological disambiguation was performed: the initial annotation was provided by PurePos, then its output was corrected manually. 

As regards morphological annotation of texts, clinical notes have special properties differing from general Hungarian, which have been considered during their analysis. 
%Beside characteristics described above, the corpus has further specialties. 
These texts contain numerous \textit{x} tokens denoting multiplication, thus they are labeled as numerals. 
Latin words and abbreviations dominate sentences, which we decided to analyze regarding their meaning. 
For instance, \textit{o.} denotes \textit{szem} `eye’ thus it is tagged as a noun (\textsc{n.nom}). 
Further on, medicines brand names are common as well, which were almost always found to be singular nouns. 
Finally, numerous sentences lack final punctuation marks that are not recovered in the test corpus. 

The manually annotated corpus was split into two parts (cf. Table \ref{tab:clin_corpus}) for our experiments. 
The first one was employed for development purposes, while new methods were evaluated on the second part.
%

\begin{table}[H]
\centering
\caption{Size of the clinical corpus created}
\label{tab:clin_corpus}
\begin{tabular}{ l  r  r } 
\hline
& Sentences & Tokens \\
\hline
Development set & 240 & 2,230 \\
Test set & 333 & 3,155 \\
\hline
\end{tabular}
\end{table}


%Further on, special properties of clinical texts need to be considered. 
These records are created in a special environment, thus they differ from general Hungarian in several aspects (cf. \cite{Orosz2013a,Siklosi2013b,Siklosi2012}):

\begin{enumerate} %[\itshape a\upshape)]
 \item notes contain a lot of erroneously spelled words,
 \item sentences generally lack punctuation marks and sentence initial capitalization, 
 \item measurements are frequent and have plenty of different (erroneous) forms,
 \item a lot of (non-standard) abbreviations occur in such texts and
 \item numerous medical terms are used originating from Latin.
\end{enumerate}

\subsection{The baseline setting and its most common errors}
\label{sec:baseline}

We built a baseline chain and analyzed its errors to improve the overall annotation quality.
It uses the Humor analyzer, which produces \emph{(morpho-syntactic tag, lemma)} pairs as analyses. 
(The output of the \acrshort{ma} is extended with the new analysis of the \textit{x} token to fit the corpus to be tagged.)
Further on, analysis candidates are disambiguated by PurePos. 

This baseline tagger produces 86.61\% token accuracy\footnote{Precision is calculated considering correct full analyses of tokens, not counting punctuation marks.} on the development set, which is remarkably lower than tagging results for general Hungarian using the same components (96--98\% as in \cite{Orosz2013b,zsibrata2013magyarlanc}). 
Further on, sentence-based precision shows that less than the third (28.33\%) of the sentences were tagged correctly. 
This fact indicates that the models of the baseline algorithm alone are weak for this task. 
Therefore, we investigated the most common errors of the chain.

\begin{table}[H]
\centering
\caption{Distribution of the most frequent error types caused by the baseline algorithm (measured on the development set)}
\label{tab:error_types}
\begin{tabular}{ l r r } 
\hline
Class & Frequency & Ratio \\
\hline
Abbreviations and acronyms & 119 & 49.17\% \\
Out-of-vocabulary words & 66 & 27.27\% \\
Domain-specific PoS of word forms & 36 & 14.88\% \\
% Numbers & 0.02\% \\
% Other & 15 & 0.06\% \\
\hline
\end{tabular}
\end{table}

Table \ref{tab:error_types} shows that the top error class is composed of mistagged abbreviations and acronyms. 
A reason for this is that most of the abbreviated tokens are previously not seen by the tagger.
Therefore, their labels are produced by the tool's guesser module, which is not prepared for handling such tokens. 
What is more, these abbreviations usually refer to medical terms (and their inflected forms) originating from Latin, thus differing notably from standard ones.

Another class of mistakes was caused by \acrlong{oov} words. 
These are specific to the clinical domain and often originate from Latin.
Although this observation is in accordance with the \acrshort{pos} tagging results for medical English, listing of such terms' analyses is not a satisfactory solution to the problem, since the number of inflected forms is significantly larger compared to English. 

Finally, domain-specific usage of some words leads the tagger astray as well. 
An example is the class participles which are mislabeled as past tense verbs. 
E.g. \textit{javasolt} `suggested’  and  \textit{felírt} `written’ are common words in the corpus, but have different \acrshort{pos} tag distributions in this domain. 
Further on, several erroneous tags are due to the lexical ambiguity being present in Hungarian (such as \textit{szembe} which can refer to `into an eye’ or `toward/against’). 

%In sum, our investigation shows that most of the errors of the baseline system can be classified into the three categories shown in Table \ref{tab:error_types}. 
Based on the classification of errors above, domain-adaptation techniques were introduced enhancing the overall accuracy of the chain.

\subsection{Domain adaptation experiments}

%Systematic changes are carried out to improve the accuracy of the chain. 
%In doing so, each enhancement is evaluated against the development corpus to monitor their usefulness.

\subsubsection{Utilizing an extended morphological lexicon}
\label{sec:ma-extension}

Supervised tagging algorithms commonly use augmented lexicons reducing the number of out-of-vocabulary words (see Section \ref{sec:biomed_tag}). 
In the case of Hungarian, this must be performed at the level of the \acrlong{ma}, since inflection is a momentous phenomenon. 
Extension of the lexicon was carried out by Attila Novák \cite{Orosz2014} adding 40,000 different lemmata to the analyzer. 
For this, he used a spelling dictionary of medical terms \cite{Fabian1992} and a freely available list of medicines \cite{Foigazgatosag2012}.
By employing the enhanced lexicon, the ratio of \acrshort{oov} words was reduced to 26.19\% (from 34.57\%) that also improved the overall accuracy to 92.41\% (on the development set). 
Further on, the medical dictionary \cite{Fabian1992} used contained numerous abbreviated tokens as well, thus the usage of the augmented analyzer also helped to decrease the number of mistagged abbreviations.

\subsubsection{Dealing with acronyms and abbreviations}

Despite improvements above, numerous errors made by the enhanced tagger were still connected to abbreviations. 
Thus, we investigated the erroneous tags of abbreviated terms first, then methods were introduced for improving the performance of the disambiguation chain. 

A detailed examination revealed that some erroneous tags were due to the over-generating nature of Humor. 
To fix such problems, we applied a simple filtering method. 
An analysis of a word with an attached full stop was considered to be a false candidate if the lemma candidate is not an abbreviation. 
Consequently, the overall accuracy was increased notably, reducing the number of errors on the development set by 9.20\%.
%(cf. ``Filtering'' in Table \ref{tab:abbrev_fixes}).

Another typical error type was the mistagging of unknown acronyms. 
Since PurePos did not employ features  dealing with such cases, these tokens were usually left to the suffix guesser resulting in incorrect annotation. 
In addition, our investigation shows that acronyms should be tagged as singular nouns in most of the cases. 
To annotate them properly, a pattern matching component was developed relying on surface features.
% (see ``Acronyms'' in Table \ref{tab:abbrev_fixes}). 

Finally, the rest of the errors were connected to those abbreviations which were both unknown to the analyzer and had not been seen previously. 
Therefore, the abbreviations labels was compared to that of the Szeged Corpus (see Table \ref{tab:pos_distribution} below).
While there are common properties between the two datasets (such as the ratio of adverbs), discrepancies are more significant. 
The most important difference is the proportions of adjectives: it is notably higher in the medical domain than in general Hungarian. 
Moreover, these values are more expressive if we consider that 10.85\% of the tokens are abbreviated in the development set, while the same ratio is only 0.37\% in the Szeged Corpus. 

\begin{table}[H]
\centering
\caption{Morpho-syntactic tag frequencies of abbreviations on the development set}
\label{tab:pos_distribution}
\begin{tabular}{ l r r} 
\hline
Tag & Clinical texts & Szeged Corpus  \\ 
\hline
\scshape{n.nom} & 67.37\% & 78.18\% \\
\scshape{a.nom} & 19.07\% & 3.96\% \\
\scshape{conj} & 1.27\% & 0.50\% \\
\scshape{adv} & 10.17\% & 11.86\% \\
Other & 2.12\% & 5.50\% \\
\hline
\end{tabular}
\end{table}

Since the nominal noun tag is the most frequent amongst abbreviations, a plausible method (``UnkN'') was to assign the \textsc{n.nom} label to unknown ones. 
Meanwhile, we kept the original word forms as lemmata. 
Although this approach is rather simple, it resulted in a surprisingly high (31.54\%) error rate reduction (cf. Table \ref{tab:abbrev_fixes}). 

\begin{table}[H]
\centering
\caption{Accuracy scores of the abbreviation handling improvements on the development set}
\label{tab:abbrev_fixes}
\begin{tabular}{ l l r } 
\hline
ID & Method &  Accuracy \\
\hline
0 & Medical lexicon & 90.11\% \\
1 & 0 + Filtering & 91.02\% \\
2 & 1 + Acronyms & 91.41\% \\
3 & 2 + UnkN & \underline{94.12\%} \\
4 & 2 + UnkUni & 92.82\% \\
5 & 2 + UnkMLE & 94.01\% \\
\hline
\end{tabular}
\end{table}

Next, we tried to approximate the analyses of abbreviations with the distribution of tags observed in Table \ref{tab:pos_distribution}. 
First, we utilized (``UnkUni'') a uniform distribution over their labels. % using the development set.
The labels \textsc{a.nom}, \textsc{a.pro}, \textsc{adv}, \textsc{conj}, \textsc{n.nom}, \textsc{v.3sg} and \textsc{v.pst\_ptcl} were used with equal probability as a sort of guessing algorithm.

Beside these, another reasonable method was to employ \acrlong{mle} for calculating a priori probabilities of labels (``UnkMLE''). 
In that way, relative frequency estimates were computed for all the tags enlisted above.

Comparing the performance of these enhancements (cf. Table \ref{tab:abbrev_fixes}), we found that this approach can also increase the overall performance, but the simple ``UnkN'' performs the best.
This can be due to the fact that the data available could be insufficient for estimating probability distribution of labels properly.

\subsubsection{Choosing the proper training data}

Since many studies showed (cf. Section \ref{sec:biomed_tag}) that the training data set significantly affects the result of a data-driven annotation chain, we investigated sub-corpora of the Szeged Corpus. 
Several properties (cf. Table \ref{tab:subcorpora_attrib}) were examined\footnote{Measurements regarding the development set were calculated manually where it was necessary.} to find a decent domain to learn from for tagging clinical Hungarian. 

\begin{table}[H]
\centering
\caption{Comparing Szeged Corpus with clinical texts}
\label{tab:subcorpora_attrib}
\begin{tabular}{ l r r r r r } 
\hline
\multicolumn{1}{l}{\multirow{2}{*}{Corpus}} & \multicolumn{1}{c}{Avg. sent.} & \multicolumn{1}{c}{Abbrev.}  &  \multicolumn{1}{c}{Unknown} & \multicolumn{2}{c}{Perplexity} \\
 & \multicolumn{1}{c}{length} & \multicolumn{1}{c}{ratio} &  \multicolumn{1}{c}{ratio} & \multicolumn{1}{c}{Words} & \multicolumn{1}{c}{Tags} \\
\hline
Szeged Corpus & 16.82 & 0.37\%  & \underline{1.78\%} & 2318.02 & 22.56\\
\hspace{0.2cm} Fiction & 12.30 & 0.10\% & 2.44\% & 995.57 & 32.57\\
\hspace{0.2cm} Compositions & 13.22 & 0.14\% & 2.29\% & 1,335.90 & 30.78\\
\hspace{0.2cm} Computer & 20.75 & 0.14\% & 2.34\% & 854.11 & 22.89\\
\hspace{0.2cm} Newspaper & 21.05 & 0.20\% & 2.10\% & 1,284.89 & \underline{22.08}\\
\hspace{0.2cm} Law & 23.64 & 1.43\% & 2.74\% & \underline{824.42} & 29.79\\
\hspace{0.2cm} Short business news & 23.28 & 0.91\% & 2.50\% & 859.33 & 27.88\\
% \hline
Development set & 9.29 & 10.85\% & -- & -- & -- \\
\hline
\end{tabular}
\end{table}

First of all, an important attribute of texts is the length of sentences. 
Shorter sentences tend to have simpler grammatical structure, while longer ones are grammatically more complex. 
Further on, clinical texts have a vast amount of abbreviations, thus their ratio can also serve as a relevant metric. 
In addition, the accuracy of a tagging system depends on the ratio of unknown words heavily, therefore their proportions were calculated. 
For this, we measured the ratio of \acrshort{oov} words on the development set. 
%In doing so, the vocabularies of training sets are compared to the training corpora (see Table \ref{tab:subcorpora_attrib}). 

Perplexity was also computed, since it can measure similarities of texts \cite{kilgarriff1998measures}. 
The calculation was carried out as follows: trigram models of word and tag sequences were trained on each corpus using Kneser-Ney smoothing, then all of them were evaluated on the development set\footnote{We used the SRILM toolkit \cite{stolcke2002srilm} for training models and measuring perplexity.}.

Our examination shows that neither none of the parts of the Szeged Corpus contains as much abbreviated terms as clinical texts have. 
Likewise, sentences written by clinicians are significantly shorter than those of the Szeged Corpus. 
Neither the calculations above, nor the ratio of unknown words suggests using any of the  subcorpora for training. 
However, the perplexity scores contradict: sentences from the law domain share the most phrases with clinical notes, while news texts have the most similar grammatical structures. 

\begin{table}[H]
\centering
\caption{Evaluation of the tagger on the development set trained with domain-specific subcorpora of the Szeged Corpus}
\label{tab:eval_subcorpora}
\begin{tabular}{ l r } 
\hline
Corpus & Morph. disambiguation accuracy \\
\hline
Szeged Corpus & \underline{94.73\%} \\
\hspace{0.2cm} Fiction & 92.01\% \\
\hspace{0.2cm} Compositions & 91.97\% \\
\hspace{0.2cm} Computer & 92.73\% \\
\hspace{0.2cm} Newspaper & \underline{93.29\%} \\
\hspace{0.2cm} Law & 92.17\% \\
\hspace{0.2cm} Short business news & 92.69\% \\
\hline
\end{tabular}
\end{table}


Since similarity measurements were not in accordance with each other, all sub-corpora were tested as training data for tagging clinical texts. 
(These experiments were performed using the previously enhanced tagging chain.)
The accuracy scores of taggers (cf. Table  \ref{tab:eval_subcorpora}) on the development set show that training on a subcorpus cannot improve the performance. 
Therefore, we decided to use the whole Szeged Corpus to train our system.

\subsection{Evaluation}

The improved chain (cf. Table \ref{tab:improvements}) was evaluated by investigating the part-of-speech tagging, lemmatization and the whole morphological annotation performance.

\begin{table}[H]
\centering
\caption{Accuracy scores of the improved tagger on the test set}
\label{tab:improvements}
\begin{tabular}{ l l r r r} 
\hline
ID & Method & PoS tagging & Lemmatization & Morph. disambig. \\
\hline
0 & Baseline system & 90.57\% & 93.54\% & 88.09\% \\
1 & 0 + Lexicon extension & 93.89\% & 96.24\% & 92.41\% \\
2 & 1 + Handling abbreviations & \underline{94.81\%} & \underline{97.60\%} & \underline{93.73\%} \\
%3 & 2 + Training data selection & 94.25\% & 97.36\% & 93.29\% \\
\hline
\end{tabular}
\end{table}

First of all, results show that the baseline method annotated almost 12\% of the tokens erroneously, while our enhancements raised the ceiling of the full morphological tagging accuracy to 93.73\%.
Therefore, we managed to eliminate almost half (47.36\%) of the errors. 
Next, precision scores also indicate that the error rate reduction is mainly due to the extended lexicon.
However, the better handling of abbreviations also increased the performance significantly (Wilcoxon test of paired samples, p < 0.05).
Therefore, our improvements yielded a system having satisfactory performance for morphologically parsing clinical texts.


This study revealed that abbreviations and \acrlong{oov} words cause the most of the errors for tagging Hungarian clinical texts.
We introduced numerous enhancements dealing with them, although not all of them were successful.
This could be due to the small amount of annotated data used inhibiting the better modeling of the domain.
%Further on, Hungarian is a less-resource language and as such we did not manage to find a better (sub)corpus for tagging clinical Hungarian than the whole Szeged Corpus. 
