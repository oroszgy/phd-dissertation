\chapter{Algorithms for full morphological disambiguation}
\section{Motivation}

This chapter draws on research in morphological tagging of languages which are so-called less-resourced (those that lack linguistic resources e.g. manually annotated corpora).
Although several attempts have been made on developing part-of-speech tagging algorithms since the 1960’s (e.g.~\cite{}), those were focusing mainly on English.
Further on, in contrast to the consensus of NLP scientist that these studies resulted in methods that commonly accepted as solutions to the problem (~\cite{}), there are still many open questions.
Most of the previous work only concentrate on increasing the accuracy of taggers on news text, many other problems (such as domain adaptation of taggers) are still barely touched.
However, there has been an increasing interest recently on processing texts in less-resourced languages, which are usually morphologically rich.
Most of them are inflectional or agglutinative which pose new challenges to researchers.
This work will give an account of agglutinating languages by investigating the case of Hungarian.

The data sparseness issue of the tagging task is remarkable for morphologically rich languages.
If we compare (cf.~\cite{Oravecz}) agglutinative languages like Hungarian or Finnish with English in terms of the coverage of vocabulary by a corpus of a given size, we find that although there are a lot more different word forms in the corpus, these still cover a much smaller percentage of possible word forms of the lemmata in the corpus than in the case of English.
On the one hand, a 10 million word English corpus has less than 100,000 different word forms~\cite{}, a corpus of the same size for Finnish or Hungarian contains well over 800,000.
On the other hand, while an open class English word has about 4--6 different inflected forms, it has several hundred or thousand different productively suffixed forms in agglutinative languages.
Moreover, there are much more different possible morphosyntactic tags in the case of these languages (cf.~\cite{}) than in English (several thousand vs. a few dozen).
Thus the problem is threefold:

\begin{itemize}
  \item an overwhelming majority of possible word forms of lemmata occurring in the corpus is totally absent,
  \item word forms that do occur in the corpus have much fewer occurrences, and
  \item there are also much fewer examples of tag sequences, what is more, many tags may not occur in the corpus at all.
\end{itemize}
Another issue for morphologically rich languages is that tagging words with only their part-of-speech is insufficient.
On one hand, it is due to the fact that the complex morphosyntactic features carried by the inflectional morphemes could not be represented in such small tagsets that have less than a hundred different labels.
On the other hand, morphosyntactic tagging is still just a subtask of the full morphological disambiguation.
In addition to a full morphosyntactic tag, the lemma of each word also needs to be identified.
Although several studies have revealed that dictionary- or rule-based lemmatization/stemming methods yields acceptable results~\cite{} for morphologically not very rich languages like English, ambiguity is present in lemmatization for highly inflectional and agglutinative languages~\cite{Hajic,Szlovénok,Morfette}.
Therefore, the identification of the correct lemma is a challenging task.
Yet, most of the currently available taggers only concentrate on determining the morphological tag but not the lemma, thus doing just half of the job.

Another key point is the case of resource-scarce languages where the annotated corpora available is limited.
Pursuing this further, to create a morphologically annotated corpus for such a language, an iterative workflow is a feasible approach:

\begin{itemize}
  \item first, a very small subset of the corpus is disambiguated manually,
  \item then a tagger is trained on this subset,
  \item after that, another subset of the corpus is tagged automatically and corrected manually, yielding a new, bigger training corpus and this process is repeated. 
\end{itemize}
For this process to be in fact feasible, a high accuracy tagger is needed which performs well even when just a small amount of training data is available. 

In this chapter, approaches on developing effective morphological tagging algorithms is presented.
First, I review common tagging and lemmatization methods, then introduce  a hybrid approach to the full.
It is being show, that the methodology presented achieves state-of-the-art accuracy, even more performs well when only a limited training data is available.
In section \ref{} a tagger combination method is presented which is built on top of two systems raising the ceiling of the tagging accuracy.
Finally, new scientific results are summed up by declaring theses.

\section{Hybrid morphological tagging}

\subsection{Background}

\subsubsection{PoS tagging}

Part-of-speech tagging is a well-known problem of natural language processing First attempts date back to the early 60’s, when mostly linguistic approaches~\cite{} were successful.
Recently, data-driven methods dominate the literature, which became popular~\cite{claws} in the 70’s due to the growing amount of annotated corpora.
During the past 50 years huge amount of attempts have been published that is impossible to review all of them.
Therefore, only the most important machine learning methods are introduced which are used nowadays. 

\paragraph{Overview of general methods}

Most of the data-driven algorithms rely on supervised machine learning algorithms which usually observe the task as a classification problem where words need to be labeled with their part-of-speech class.
Although there are attempts~\cite{} which classify words locally, most of the studies handling tagging as a sequence classification problem (e.g.~\cite{}), where the goal is to predict series of labels for sentences.
As a result of such methodologies, the basic unit of PoS tagging is the sentence not the word.
Further on, sequence tagging tasks are often solved using statistical modelling techniques.
Such attempts are feasible when a huge amount of annotated data is available.
In such cases a decent method can learn generalize well from the corpus yielding a knowledge that can be applied effectively.
Statistical techniques can be further categorized as generative or discriminative learners.
While generative classifiers model the joint probability $p(x,y)$ and predicts $y$ labels for $x$ inputs by estimating the $p(x|y)$ conditional probability using the Bayes rule, discriminative algorithms learn models for $p(y|x)$ directly.

Hidden Markov models are a notable part of generative models.
From this point of view the tagging task can be formulated by searching a tag sequence for the words which maximize the joint probability (cf. equation \eqref{}).
With the application of the Bayes-rule (see \eqref{}) the model is decomposed to a lexical model ($P(w_1^n|t_1^n)$) and a tag transition model ($P(t_1^n)$).

\begin{equation}
\argmax_{t_1^n} P(w_1^n, t_1^n)
\end{equation}

\begin{equation}
\argmax_{t_1^n}  P(w_1^n|t_1^n) P(t_1^n)
\end{equation}

\begin{equation}
P(w_1^n|t_1^n) \approx \prod_i P(w_i|t_i)
\end{equation}

\begin{equation}
P(t_1^n) \approx \prod_i P(t_i|t_{i-1}, t_{i-2}, \dots, t_{i-k})
\end{equation}

Since the model still cannot be directly estimated in that form, markovian assumptions (see equations \eqref{,}) are utilized to simplify the problem. A special case of this is when $k=2$, then is is usually referred as trigram tagging. A successful application of the latter approach was first introduced by Brants \cite{} in 2000. Since then, his tool (TnT) has been used (such as \cite{}) and adapted several times (e.g. \cite{IceNLP,Finntagger,Hunpos,Oravecz}). TnT is based on interpolated trigram transition estimates which is combined with a case sensitive unigram lexical model. Further on, to perform better for unknown words it incorporates a suffix based part-of-speech guessing algorithm as well. As usual for HMM-based methods, TnT-like tools decode their model with the Viterbi algorithm. 

There is also a large volume of published research describing the application of discriminative models. One of the most popular methodologies is the maximum entropy framework that was introduced for NLP almost twenty years ago by Ratnaparkhi \cite{}. Systems utilizing that approach estimate the probability mass for tag sequences directly (cf. equation \eqref{}).

\begin{equation}
\argmax_{t_1^n} P(t_1^n | w_1^n)
\end{equation}

The maximization task is reformulated applying markovian assumptions (see equation \eqref{}). Hence, this can be calculated directly using logistic regression (cf.  equation \eqref{}). For the best performance, studies suggest using diverse features ($f_i$) indicating a special property of the context.. Such indicators are weighted with parameters ($\lambda_i$), which are usually iteratively estimated. Finally, decoding is mostly carried out with beam search.

\begin{equation}
P(t_1^n | w_1^n) \approx \prod_i p(t_i|h_i)
\end{equation}

\begin{equation}
P(t|h) = \frac{\exp{\sum_j{\lambda_j f_j(t,h)}}}{\sum_{t^\prime}\exp{\sum_j{\lambda_j f_j(t^\prime,h)}}}
\end{equation}

Over the years numerous applications and tools have grown out from the work of Ratnaparkhi.
A popular and efficient successor of his system is the Stanford tagger~\cite{}.
This tool extends the original model by increasing the context to the right thus resulting in a so called cyclic dependency network.
It achieves state-of-the-art performance for English~\cite{} and many other languages~\cite{}.
Another closely related approach is the conditional random field methodology.
Such algorithms generalizes the idea behind the maximum entropy framework to overcome its limitations (for details see~\cite{}).
The main difference is that such approaches use a global exponential model for yielding better probability estimates.

Beside sequence labeling approaches, other generic machine learning methods were also successfully to the tagging task.
There are applications using Support Vector Machines~\cite{SVMTAgger}, Decision list classifiers~\cite{}, Decision Tree learners~\cite{lengyel}, Instance Based algorithms~\cite{lengyel,...} , furthermore even perceptron learning~\cite{lengyel,collins} and neural networks were applied recently~\cite{from_scratch,Collins}.
Depending on the architecture, most of the models incorporates well-known features such as neighbouring words, suffixes, neighbouring tags.
These are essential indicators, which leads the way for a tagging algorithm.

Beside pure statistical algorithms, linguistic or mixed (hybrid) approaches can deal successfully with the task as well.
A good is example is Brill’s transformation based learning~\cite{} which is a mixture of data-driven and symbolic approaches.
The algorithm needs transformation rule templates to be input, but learns disambiguation rules from the training data automatically.
Another sort of hybridization methods mix statistical learning methods with tag dictionaries, lexicons or hand-written rules.
They are usually utilized to overcome data-sparseness difficulties.
An example for this is the work of Hajic et al.~\cite{} and, where they propose investing in the creation of hand-crafted morphological lexicons instead of more annotated data.
As we see below, these linguistic resources are even more important in the case of a morphologically rich languages.

\paragraph{Applications to morphologically rich languages}

Recently, more and more studies got published adapting a ML methods for inflectional/agglutinative languages.
The model applied greatly varies, but they share some things in common that is all of them face the rich nature of the target language morphology.
Hence, these attempts usually target either the increased number of out-of-vocabulary wordforms or the large size and complexity of the tagset used.

In the next, I review the common methodologies for morphosyntactic tagging of morphologically rich languages in particular the question how they overcome issues caused by morphologically richness.

Starting with Polish, which is a highly inflectional language, there numerous attempts have been published recently~\cite{}, however their accuracy values are below the average.
Almost all the studies rely on a morphological analyzer to get possible morphosyntactic tag candidates, thus reduce the search space of the decoder.
Another popular approach is to utilize tiered tagging to handle the rich tagset~\cite{}.
As a result, such methods not aim at computing the whole morphosyntactic description in one step, rather resolving its parts one after another.
Considering the algorithms they use, the range of applications is wide.
Beside an adaptation of Brill’s tagger~\cite{}, C4.5 decision trees~\cite{}, the memory-based learning principle~\cite{} and CRF models are employed~\cite{} as well. 

Further on, morphological tagging methods for Czech are also an important part of the literature.
The first successful attempt were published by Hajic and Hladká~\cite{Hajic1998a} who based their algorithm on a discriminative model.
This approach uses a morphological analyzer and builds individual prediction models for ambiguity classes.
Actually, the best results for Czech were obtained using the combination of numerous systems~\cite{Hajic2007}.
They used hree different statistical tagger (HMM, Maximum Entropy and Averaged perceptron), moreover symbolic components are incorporated as well.
A MA is used to compute the possible analyses, while the rule-based disambiguator tool removes agrammatical tag sequences. 

Amongst studies on tagging inflectional languages, Stanford tagger plays an important role as well.
Its flexible architecture allows researchers to incorporate various morphological features, which may boost the system’s performance.
An example for this is~\cite{} which presents a hybrid system while using an extended feature set.
It incorporates a lexicon, and further disambiguation rules as well.
Beside discriminative methods, applications of trigram tagging methods have been demonstrated to perform well (for Danish~\cite{}, Croatian~\cite{}, Slovenian~\cite{}, Icelandic~\cite{}).
In such systems, the coverage of the lexicon used and the unknown word guessing module is crucial. 

In the case of agglutinating languages, where thousands of different wordforms can exist for the same lemma, the usage of finite state methods is indispensable.
An example is the work of Silferberg and Lindén~\cite{}, in which an HMM-based, weighted finite state tagger for Finnish~\cite{}.
As regards Turkish, Daybelge and Cicelki use~\cite{} a rule-based finite-state system.
Further on, there are other attempts which incorporate a morphological analyzer to statistical learning methods.
Examples are the application of the perceptron learning principle (Sak et al.~\cite{}) and the approach of Dilek et al.~\cite{} which uses trigram tagging is as a base.

As regards Hungarian, the list of previous studies is considerable.
In a chronological order, Megyesi adapted Brill’s transformation based method~\cite{} in 1998 but not used any morphological lexicon, thus her approach resulted in moderate success.
Similarly, Horváth et al. experienced with pure machine learning algorithms (such as C4.5 or instance based learning), but their best system’ accuracy was much below 90\%.
Three years later, the first promising approach was presented by Oravecz and Dienes~\cite{}, who utilized TnT with a weighted finite-state lexical model.
In 2006, Halácsy et al. investigated an augmented maxent model~\cite{} in combination with language specific morphological features and a MA.
In this study, their best result was achieved by combining the latter model with a trigram based tagger.
Later, they created the HunPos system~\cite{} that reimplements and extends TnT.
The precision of their results (with a morphological lexicon) is in pair with the one of Oravecz and Dienes~\cite{}.
Moreover, the tool is proven to be performs well on English as well.
Next, Kuba et al. experienced with boosting and bagging techniques for transformation based learning.
Although they managed to reduce the error rate of the baseline tagger, they failed to reach the accuracy of previous approaches.
Recently, Zsibrita et al. published magyarlanc~\cite{} which is a natural language processing chain, involving a morphological tagger as well.
They adapted the Stanford tagger augmenting the chain with the analyses of a morphological analyzer. 

In sum, there are several successful approaches for Hungarian relying on either a discriminative or a generative model, but all of them using a high coverage morphological analyzer.
Likewise, most of the successful approaches aiming a morphologically rich language rely on a morphological analyzer or lexicon.
Beside this, handling of unknown words is crucial point as well, therefore either a guessing module or morphological features widely applied as well. 

\subsubsection{Morphological disambiguation}

In contrast to PoS tagging, there have been insufficient discussion about full morphological tagging in recent studies of natural language technology.
The reason behind this is that most of the work concentrate on morphologically not very rich languages (such as English), where PoS tagging is generally sufficient and the ambiguity of the lemmatization task is negligible.

Nevertheless, studies published on morphological tagging can be grouped depending on their lemmatization approaches.

\begin{itemize}
  \item First of all, there are attempts (even for inflectional languages) which simply ignore the task of lemmatization (such as~\cite{Hajic,Tufis(román),FinnHunPosos}) and performs only the morphosyntactic tagging task. 
  \item Next, numerous researchers propose a two stage model, where the first stage is responsible for finding the full morphosyntactic tag, while the second one is for identifying the lemma for a given word, tag pair.
For example, Erjavec and Dzeroski decompose the problem~\cite{} by utilizing a trigram a tagger first, then using a decision list lemmatizer.
Further on, Agic et al. build their system~\cite{} in top of HunPos, and calculates the lemmata with a data-driven rule-based lemmatizer~\cite{}. 
  \item Another feasible approach is to treat the tagging task as a disambiguation problem where a morphological analyzer generates the possible analyses (involving lemmata) for words, then the disambiguator method selects the correct analysis.
This approach is typical e.g. for Turkish (cf.~\cite{török_cikkek}) studies, but similar studies are also published for Czech (such as~\cite{valamelyikHajic}) as well.
  \item Finally, the task can be handled as a unified tagging problem.
Recently, Chrupala et al. suggested~\cite{} a unique approach, in which they employ a joint architecture which tags words with their tag and lemma at the same time.
Their algorithm treats lemmatization as a classification problem, where the proper class of \emph{(wordform, tag)} pair is the transformation sequence that is needed to get the lemma.
Further on, they build on the maximum entropy principle employing separate models for the subtasks, however decoding jointly with beam search.
Another similar approach is presented by Laki and Orosz~\cite{}.
Their system (HuLaPos) merges the PoS labels with the transformation sequences to a unified tag, moreover it uses the Moses framework to train an HMM-like tagger.
\end{itemize}
Considering the second case, where the task is performed in two phases, lemmatization can be carried out using various methods.
Beside using rules and dictionaries (e.g.~\cite{}), a rather simple method is to select the most frequent lemma for the (word, tag) pair.
Next, there are studies (for Slovenian) employing~\cite{} and improving~\cite{} the RDR framework.
This approach relies on C4.5 decision trees learning suffix transformation for wordforms but ignoring PoS tags.
An advanced transformation based method (CST) has been introduced~\cite{} recently, which handles morphological changes in pre-, in- and suffixes as well.
What is more, It has been also successfully employed for many inflectional languages (e.g.~\cite{}).

As described above, a commonly utilized toolkit for Hungarian, is the magyarlanc.
This system is able to lemmatize words, but does this in a simple way: it assigns the most common lemmata (depending on the morphosyntactic label) to known words.
However, the tool contains an unknwon word guesser as well relying on language-specific linguistic rules.
Moving on, HuLaPos was developed in a language independent way, but it can be applied to morphologically rich languages providing an well-established performance.
Unfortunately other Hungarian tagging approaches ignores lemmatization thus leaving the task halfly solved.

In this work I present a complex method that aims to tag texts with their full morphosyntactic tag and lemma.
What differentiate it from few other existing tools that it bears with a high precision for agglutinating languages, even more this accuracy remains when just a few amount of training data is available.
The system described lies on a hybrid architecture, thus allowing its easy domain adaptation to new domains. 

\subsection{Methods for morphological tagging }

In this section, a complex algorityhm is presented that is capable of morphological tagging with high accuracy.
The system created (called PurePos) is a hybrid one, which relies on numerous statistical models and a morphological analyzer component.
Furthermore, it can effectively incorporate linguistic rules as well.
Its architecture (cf. figure \ref{}) is consistent with those of the other studies~\cite{} proposing a two-staged model for the full tagging task.
The data flow starts from the MA: having morphological analyses (possible (lemma, tag) pairs), a trigram model is used to tag the sentences with morphosyntactic labels, then the lemmatization process is carried out.
However, the process can be modified by manipulating either the output of the analyzer or the result of the tagging process.

\subsubsection{The PoS tagging model}

My goal was to build a system which is not only highly accurate but has a short training time as well.
Fast turnaround time is e.g. needed in the iterative corpus creation scenario described above.
In order to achieve high accuracy and fast training time, PurePos builds on HMM-based methods introduced in TnT and HunPos.
Unlike the latter ones, it relies on morphological analyses hence probability estimates are calculated for analyses only permitted by the MA.
As regards the statistical model, PoS tagging is carried out by calculating
$$\argmax_{t_1^m} \prod_{i=1}^m P(t_i | t_{i-1}^{i-n}) P(w_i|t_{i-1}^{i-n})$$
for $w_1 \dots w_n$ words. 

The contextual model is an interpolated \emph{\emph{n}}-gram model (cf. equation \eqref{}), where each of its component is estimated with maximum likelihood estimation (see equation \eqref{}).
Lambda parameters are calculated using deleted interpolation as it is proposed in~\cite{}.
Due to data sparseness the order of the tag transition model are usually set to 3.

\begin{equation} 
P(t_i | t_{i-1}^{i-n}) = \sum_{k=0}^{n-1} \lambda_k \hat{P}(t_i|t_{i-1}^{i-k})
\end{equation}

\begin{align} 
\hat{P}(t_i|t_{i-1}^{i-k}) = \frac{c(t^{i-k}_i)}{c(t_{i-1}^{i-k})} (k>0) \\
\hat{P}(t_i) = \frac{c(t_i)}{N} (k=0)
\end{align}

The lexical model is composed of two main components. The first one is used for tagging tokens previously seen in the training data, while the second is for guessing word classes for unknown words. In fact, each component is doubled, since separate models are maintained for both uppercase and lowercase words as well. 

Handling of previously seen words is carried out using similar techniques seen introduced for the contextual component. The difference is the target value to be estimated:

\begin{equation} 
P(w_i | t_{i}^{i-n}) = \sum_{k=0}^{n-1} \lambda_k \hat{P}(w_i|t_{i}^{i-k})
\end{equation}

$\hat{P}(w_i|t_{i}^{i-k})$ is again computed relying on MLE and deleted interpolation (as seen above) . Best results are usually achieved setting $k$ to 2 for equation \eqref{}.

As regards tagging of unknown words, estimating their PoS label the distribution of rare token’s tags are used. Suffixes are strong predictors for PoS tags in the case of agglutinative languages (cf. []), therefore PurePos is built to exhaust such indicators. Algorithms applied are derived from Brants []. The algorithm uses $\{s_{n-l+1} \dots s_n\}$ $l$ long suffixes to build the recursive formula for estimation:
\begin{align}
 P(t|s_{n-l+1}, \dots, l_n) 
 = \frac{ \hat{P}(t|s_{n-l+1}, \dots, l_n) + \theta_i \hat{P}(t|s_{n-l}, \dots, l_n)}{1+\theta_i}
\end{align}

Brants propose successive abstraction~\cite{} for estimating $\theta_i$ parameters, which is also applied in PurePos as well.
During the tagging process, the algorithm used is hybridized in a way that it calculates probability mass only for tags output by the analyzer.

In order to allow the usage of linguistic knowledge after the disambiguation process, beam search is employed for decoding.
This allows the tool to retrieve multiple tagging sequences for sentences involving the tagging scores as well:

\begin{equation} %%TODO: ez történik, kell ez ide?
Score(w_1^m,t_1^m) = \log \prod_{i=1}^m P(w_i|t_i,t_{i-1})P(t_i|t_{i-1},t_{i-2})P(l_i|t_i,w_i)
\end{equation}

\subsubsection{The lemmatization model}

Lemmatization is performed in two steps (cf. figure \ref{}).
First, candidates are generated for the given (word, morphosyntactic tag) pairs.
If analyses are available, lemmata of them are used, otherwise suffix based guessing is carried out.
The latter components is statistical learning method built on top of the tag-guessing model.
However, in order to efficiently store \emph{(lemma, tag)} pairs, they are represented as suffix-transformation strings which are to be performed to get the lemma from the word form in case of the given tag (for an example see tabe \ref{}).
During its training phase, frequencies are collected for \emph{(word, tag, transformations)} triplets and a suffix model is built as described in section \ref{}.
Calling the module returns the most 20 root candidates that are considered to be the most probable.

\begin{tabular}
  Word &&  házaink my houses &&  baglyot owl\\
  Tag &&  FN.PSt1i &&  N.ACC\\
  Lemma &&  ház &&  bagoly\\
  Transformation &&  -4 &&  -4+oly\\
\end{tabular}

As for the probability estimation of lemmata, PurePos selects the lemma candidate that has the maximal probability according to following conditional model:

\begin{equation}\label{lemma-max}
\argmax_l P(l|t,w)
\end{equation}

 
