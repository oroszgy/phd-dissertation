%TODO: felütés
Hospitals produce a huge amount of clinical notes that have solely been used for archiving purposes and have generally been inaccessible to researchers. 
However, application of NLP tools could make available the hidden knowledge of archived records, thus boosting medical research. 
To be able to extract information from such textual data, they must be preprocessed properly. First adequate text segmentation methods\footnote{While the term \emph{text segmentation} is widely used for diverse tasks, in our work it is used as the process of dividing text into word tokens and sentences.} 
are required for finding token and sentence boundaries. Next, morphological tagging is indispensable for information extraction tasks of morphologically rich languages. 

There are only a few studies on processing Hungarian medical records. First, Siklósi et al. \cite{Siklosi2012,Siklosi2013} presented a system that is able to correct spelling errors in clinical notes. They use a mixture of language models to generate correction candidates, however focusing only on correctly segmented words. 
Although, a resolution method for clinical abbreviations has been presented by them \cite{Siklosi2013b}, morphosyntactic tagging is still untouched. 
As far as we know, no study investigates method for segmenting nor tagging clinical Hungarian. 

In this chapter, we present algorithms for preprocessing clinical texts effectively. Firstly, an accurate method is introduced detecting sentence and token boundaries. Secondly, tagging experiments are presented yielding a proper morphological tagger for clinical Hungarian. 

\section{Text segmentation in clinical records}

Existence of reliable segmentation methods is essential, since error propagation in a text-processing chain is usually a notable problem. In addition, this is even more remarkable in the case of clinical records, when errors are present at various levels of processing.

Even though existing segmentation methods for Hungarian performing well on general domains, they have serious difficulties on clinical texts.
These originate mainly in errors of the text involving
\begin{inparaenum}[\itshape a\upshape)]
 \item typing errors (i.e. mistyped tokens, nonexistent strings of falsely concatenated words) and
 \item nonstandard usage of Hungarian.
\end{inparaenum}
While errors of the first type can generally be corrected with a rule-based tool, others need advanced methods. 

In this section, a hybrid approach to normalization and segmentation of Hungarian clinical records is presented. 
The method consists of three phases: first, a rule-based clean-up step is performed; then tokens are partially segmented; finally, sentence boundaries are determined. 
We start with detailing the tool's architecture. Then, key elements of the sentence boundary detection (SBD) algorithm are described. 
Finally, the system presented is evaluated against a gold standard corpus, and is being compared with other tools available.

\subsection{Previous work on text segmentation}

Even though, numerous approaches deal with English noisy texts, only a few attempts have been made (cf. \cite{Siklosi2012,Siklosi2013,Siklosi2013b}) for Hungarian. Latter studies completely ignores the segmentation problem, what is more, most of the attempt attempts for clinical English also ignores the description of the tokenization/SBD algorithm applied.

The task of text segmentation is often composed of several subtasks: normalization of noisy text (when necessary), segmentation of words, and sentence boundary detection.  
(Latter subtasks usually deal with abbreviations as well, since their identification helps the identification of sentence and token boundaries.)
Although, these subtasks are generally performed one after another, there are approaches (e.g. \cite{zhu2007unified}), where text segmentation and normalization are treated as a unified tagging problem. %Further on successful approaches exist using only series of rule-based modules modules (e.g. \cite{mikheev2002periods,halacsy2004creating}). 

First of all, tokenization is generally treated as a simple engineering problem\footnote{In the case of alphabetic writing systems.} aiming to split punctuation marks from word forms. On the other side, SBD is a rather deeply researched topic. As Read et al. summarize \cite{read2012sentence}, sentence segmentation approaches fall into three classes: 
\begin{inparaenum}[\itshape 1\upshape)]
 \item rule-based methods, that employ domain- or language-specific knowledge (such as abbreviations); 
 \item supervised machine learning approaches, which, since they are trained on a manually annotated corpus, may perform poorly on other domains; 
 \item unsupervised learning methods, which extract their knowledge from raw unannotated data. 
\end{inparaenum}

As regards ML attempts, one of the first methods was presented by Riley \cite{riley1989some} applying decision-tree learners for disambiguating full stops. He utilized mainly lexical features, such as word length and case, and probabilities of a word being sentence-initial or sentence-final. 
Next, the SATZ system of Palmer et al. \cite{palmer1997adaptive} enables employing machine learning algorithms based on not contextual and PoS features as well. 
The system was successfully applied \cite{palmer1997adaptive} to several European languages using e.g. neural networks. 
Further on, the utilization of the maximum entropy learning approach for SBD was introduced by Reynar and Ratnaparkhi \cite{reynar1997maximum}: their system classifies tokens containing `.', `?' or `!' by using contextual features and a prepared abbreviation list. 
Recently, Gillick presented  \cite{gillick2009sentence} a similar approach for English by applying support vector machines.% and resulting in a state-of-the-art performance.

Besides, Mikheev presents \cite{mikheev2002periods} a small set of rules for detecting sentence boundaries (SB) with a high accuracy. 
In another system presented by him \cite{mikheev2000tagging}, the the latter method is integrated into a PoS-tagging framework enabling the classification of punctuation marks as well. These characters are labeled either as a sentence boundary, an abbreviation or both. 
Moving on, Punkt \cite{kiss2006unsupervised} is a tool presented by Kiss and Strunk employing only unsupervised machine learning techniques. They use scaled log-likelihood ratio for deciding whether a \emph{(word, period)} pair is a collocation or not.

Although tokenization and SBD tasks are trending areas in natural language processing currently, there are a few attempts aiming medical texts. 
Sentence segmentation methods, employed by clinical text processing systems, fall into two classes: many of them apply rule-based settings (e.g. \cite{XuSDJWD10}), while others employ supervised machine learning algorithms (such as \cite{apostolova2009automatic,cho2002text,Savova2010,taira2001automatic,tomanek2007sentence}).
Latter systems mainly use maximum entropy or CRF learners, thus handcrafted training corpora are essential. 
% Such corpora can be either  domain-specific or general. 
In practice, training data from a related domain yields better performance. However Tomanek et al. argue \cite{tomanek2006reappraisal} argue that the domain of the training corpus is not critical.

As regards Hungarian, there are two text segmentation tools available. Huntoken \cite{halacsy2004creating} is an open source tool based on Mikheev’s rule-based system, while \texttt{magyarlanc} \cite{zsibrata2013magyarlanc} has an adapted version of MorphAdorner’s rule-based tokenizer \cite{kumar2009monk} and sentence splitter.

As there is no training data available for clinical Hungarian, we can only utilize a system based on rules or unsupervised methods. Next, we found that existing tools cannot process medical texts properly. Therefore, we investigate special properties of the target by creating a manually segmented corpus. 

\subsection{Clinical text used}

%Our aim was to develop a high-performance text segmentation algorithm for Hungarian clinical records, since the resulting outcome is intended to be used by shallow parsing tools, which may be parts of a greater text processing system. 
To ensure the high quality of the method being developed, a gold standard corpus is created.
However, the corpus is highly erroneous, thus input texts had to be normalized first.
We had to deal with the following errors\footnote{Text normalization is performed using regular expressions.}:
\begin{enumerate}
 \item doubly converted characters, such as `\&amp;gt;',
 \item typewriter problems (e.g. `1' and `0' is written as `l' and `o'),
 \item dates and date intervals that were in various formats with or without necessary whitespaces (e.g. `2009.11.11', `06.01.08'),
 \item missing whitespaces between tokens that usually introduced various types of errors, such as:
 \begin{enumerate}
  \item measurements 
  were erroneously attached to quantities (e.g. `0.12mg'),
  \item lack of whitespace around punctuation marks (e.g. `töröközegek.Fundus:ép.'),
 \end{enumerate}
 \item various formulation of numerical expressions.
\end{enumerate}
 
%The preprocessing tool is a series of regular expressions that are based on errors found in the development set. 
%Further segmentation of normalized text is done on sentence- and token-level as well.

In order to investigate possible pitfalls of the algorithm being developed, the gold standard data was split into two sets of equal sizes: a development and a test set containing 1320 and 1310 lines respectively. 
The first part is used to identify typical problems in the corpus and to develop the segmentation methods. The second part is used to verify our results. 

The distribution of abbreviations, punctuation marks and capitalization in a certain text help to reveal the unique segmentation problems of that documents. Therefore a comparison of clinical texts and a corpus of general Hungarian (Szeged Corpus \cite{Csendes2004}) is carried out: 
\begin{enumerate}
 \item 2.68\% of tokens found in clinical corpus sample are abbreviations while the same ratio for general Hungarian is only 0.23\%; 
 \item sentences taken from the Szeged Corpus almost always end in a sentence final punctuation mark (98.96\%), while these are totally missing from clinical statements in 48.28\% of the cases; 
 \item sentence-initial capitalization is a general rule in Hungarian (99.58\% of the sentences are formulated properly in the Szeged Corpus), but its usage is not common in the case of clinicians (12.81\% of the sentences start with a word that is not capitalized); 
 \item the amount of numerical data is significant in medical records (13.50\% of sentences consist exclusively of measurement data and abbreviations), while text taken from the general domain rarely contains statements that are full of measurements. 
\end{enumerate}

\subsection{Evaluation metrics}

There are no  metrics commonly used by researchers for text segmentation tasks.
Researchers specializing in machine learning approaches prefer to calculate precision, recall and $F$-measure.
Others, having a  background in speech recognition, prefer to use NIST and Word Error Rate. 
Moving on, Read et al. have reviewed \cite{read2012sentence} the current state-of-the-art in sentence boundary detection proposing a unified metric for comparing the performance of different approaches. 
Their method allows to measure sentence boundaries at any position, since characters are labeled as sentence-finals or non sentence-finals. In doing so, simple accuracy can be used as a metric. 

In our work, we rely on the approach of Read et al. We generalize it for our task: the text is considered as a sequence of characters and empty strings. Segmentation is treated as a single classification problem, where all of the entities (either characters or empty string between them) are labeled with the following tags: 
\begin{description}
 \item[$\langle$T$\rangle$] --  if the entity is a token boundary,
 \item[$\langle$S$\rangle$] -- if it is a sentence boundary,
 \item[$\langle$None$\rangle$] -- if the entity is neither.
\end{description}
This classification scheme enables us to measure accuracy. Furthermore, we can also calculate precision, recall and $F$-scores for each label as well. 

Accuracy, as described above, is utilized to monitor the progress of the whole segmentation task in tis study.  
Further on, it is important to measure the subsystem's performance, thus precision and recall values are calculated for both word tokenization and SBD. 
Precision becomes more important than recall for segmenting sentences, since 
an erroneously split sentence may cause information loss\label{sec:loss}, while statements might still be extracted from multi-sentence text. 
Consequently, $F_{0.5}$ is computer for SBD, while word tokenization is evaluated against the $F_1$ measure. 

\subsection{Segmentation Methods}

First, we introduce a baseline rule-based method for marking word and sentence boundaries\footnote{Rules and heuristics used are formulated investigating the development corpus.}. 
Next, further extensions of that method are detailed. 

\subsubsection{Baseline word tokenization and sentence segmentation}

The baseline rule-based algorithm is composed of two parts. First, it tokenizes words (BWT) by using regular expressions implemented in standard tokenizers. However, this method does not try disambiguate periods attached to the ends of words. The reason for this is that proper handling of such words would need to recognize all the abbreviations existing in clinical texts. 

In the second phase, (BSBD) sentence segmentation is performed in a way to avoid information loss (as described in section \ref{sec:loss}.) 
In doing so, the method minimizes the possibility of making false-positive errors by splitting sentences if there is a high confidence of success. 
These cases are when:
\begin{enumerate} 
 \item a period or exclamation mark directly follows another punctuation mark token\footnote{Question marks are not considered as sentence-final punctuation marks, since they generally indicate a questionable finding in clinical texts.};
 \item a line starts with a full date, and is followed by other words (The last white-space character before the date is marked as SB.);
 \item a line begins with the name of an examination followed by a semicolon and a sequence of measurements.
\end{enumerate}

The aggregated algorithms yields 100\% precision and 73.38\% recall for the token segmentation task\footnote{The values that are presented in this section were measured on the development set.}, while the corresponding values for the sentence boundary detection are 98.48\% and 42.60\% respectively. 
The latter values mean that less than half of the sentence boundaries are discovered, which obviously needs to be improved. 
An analysis of errors shows that the tokenization module has difficulties only with sentence final periods. 
That is because words with punctuation mark are left ambiguous.
This implies that an advanced sentence boundary detection algorithm would result in higher recall scores for word tokenization as well.
%Because of these, hereunder we only concentrate on improving the BSBD module.

\subsubsection{Improvements on sentence boundary detection}\label{sec:improvements}

Usually, there are two sort of indicators for detecting sentence boundaries: 
\begin{enumerate}
 \item when a period ($\bullet$) is attached to a word a sentence boundary is found for sure only if the token is not an abbreviation;
 \item if a word starts with a capital letter and it is neither part of a proper name nor of an acronym.
\end{enumerate}

Unfortunately, these are not directly applicable in our case. First, medical abbreviations are diverse, since clinicians usually introduce new ones not being part of the standard. 
Further on, Latin words and abbreviations are sometimes capitalized by mistake and there are subclauses that start with capitalized words. 
Finally, as shown in Section \ref{}, several sentence boundaries lack both of these indicators.

However, these features can still be used finding sentences. 
A full list of possible abbreviations is not necessary. It is enough to find evidence for the separateness of a word and the attached period to mark a sentence boundary. 
Therefore we  adapt the method of Kiss and Strunk \cite{kiss2006unsupervised} that was whoen to be performing well before in similar scenarios.

Scale-log likelihood ratio was originally used for identifying collocations by Dunning \cite{dunning1993accurate}, however it has been adapted for the sentence segmentation task \cite{kiss2006unsupervised}. The basic idea of the latter application is to handle abbreviations as collocations of words and periods. In practice, this is formulated via a null hypothesis \eqref{eq:h0} and an alternative one \eqref{eq:ha}. 

\begin{equation} \label{eq:h0}
H_0: P(\bullet|w) = p = P(\bullet|\neg w)
\end{equation}
\vspace{-0.5cm}
\begin{equation} \label{eq:ha}
H_A: P(\bullet|w) = p_1 \neq p_2 = P(\bullet|\neg w) 
\end{equation}
\vspace{-0.5cm}
\begin{equation} \label{eq:loglambda}
log \lambda = -2 log \frac{L(H_0)}{L(H_A)}
\end{equation}


\eqref{eq:h0} expresses the independence of a \emph{(word, $\bullet$)} pair, while \eqref{eq:ha} formulates that their co-occurrence is not just by chance. $log \lambda$ score is calculated \eqref{eq:loglambda} computing their ratio. This new function is shown to be asymptotically $\chi^2$ distributed, therefore it can be applied as a statistical test \cite{dunning1993accurate}. Kiss and Strunk found that pure $log \lambda$ score performs poorly\footnote{In terms of precision.} in abbreviation detection scenarios, therefore they introduced scaling factors \cite{kiss2006unsupervised}. 
In that way, their approach loses the asymptotic relation to $\chi^2$ and becomes a simple filtering algorithm.

We aim to find candidates co-occurring only by chance, thus we employ the inverse of $\log\lambda$:$iscore=1/log\lambda$. Several experiments are performed on the development set that showed that the scaling factors described  below give the best performance.

In contrast to the original work \cite{kiss2006unsupervised}, counts and count ratios do not indicate properly whether a token and the period is related in a clinical record. It is because diverse abbreviations exist and their individual frequencies scores are relatively low. 
Further on, good indicators of abbreviations are their lengths ($len$):  shorter tokens tend to be abbreviations, while longer ones do not. A function formulates this observation by penalizing short words, while increasing the scores of others. 
Having a medical abbreviation list of almost 200 \label{sec:abbrev} elements\footnote{The list is gathered with an automatic algorithm on the development corpus using word shape properties and frequencies. The most frequent elements are manually verified and corrected.} 
we found that that more than 90\% of the abbreviations are shorter than three characters. This fact encouraged us to use a scaling factor as in \eqref{eq:s_l}. 
In doing so, this factor is not just able to boost the score of a pair but it can also decrease it as well. 


\begin{equation} \label{eq:s_l}
S_{length}(iscore)= iscore \cdot \exp{(len/3 -1)}
\end{equation}

Recently, HuMor \cite{Proszeky1994,Proszeky2005} -- a morphological analyzer (MA) for Hungarian -- has been extended with the content of a medical dictionary \cite{Orosz:mszny2013}. 
Therefore this could be used to enhance the sentence segmentation algorithm.  
HuMor is able to analyze possible abbreviations and full words as well, thus its output is utilized by an indicator function \eqref{eq:sign}.
We put larger weights to this factor (Equation \eqref{eq:s_m}) compared to others, since this method relies on linguistic evidence.

\begin{equation}\label{eq:sign}
 indicator_{morph}(word) =
  \begin{cases}
   1  & \text{if $word$ has an analysis of a known full word} \\
   -1 & \text{if $word$ has an analysis of a known abbreviation} \\
   0  & \text{otherwise}
  \end{cases}
\end{equation}

\begin{equation} \label{eq:s_m}
S_{morph}(iscore)= iscore \cdot \exp{( indicator_{morph} \cdot len^2)}
\end{equation}

Finally, the last indicator is based on the analysis of the development data: a hyphen is generally not present in abbreviations but rather occurs in full words. This led us to the third factor \eqref{eq:s_h} where $indicator_{hyphen}$ is $1$ only if the word contains a hyphen, otherwise it is $0$. 


\begin{equation} \label{eq:s_h}
S_{hyphen}(iscore)= iscore \cdot \exp{(indicator_{hyphen} \cdot len)}
\end{equation}


Scaled $iscore$ is calculated for all \emph{(word, $\bullet$)} pairs not followed by another punctuation mark. If this value is higher than a threshold, the period is regarded as a sentence boundary and it is detached.\footnote{Threshold value is empirically set to $1.5$.}
Investigating the performance of the method described above, it was pipelined after the BSBD module producing 77.14\% recall and 97.10\% precision on the development set.

To further improve the tool, we utilize the second source of information that is word capitalization. Therefore subsequent rule-based component was created to deal with words beginning with capital letters . 
Good SB candidates of these are the ones that do not follow a non sentence terminating\footnote{Sentence terminating punctuation marks are the period and the exclamation mark for this task.} punctuation, and are not part of a named entity. 
First, sequences of capitalized words are omitted. Then the rest is processed with HuMor. If a word does not have a proper noun analysis but is capitalized it is marked as the beginning of a sentence.  
% In our case common words are those that do not have a proper noun analysis. 
This component alone with BSBD results in 65.46\% of recall and 96.37\% of precision on the development set.

\subsection{Evaluation}

% Our hybrid algorithm has been developed using the development set, thus it is evaluated against the rest of the data. 
% %As the starting point of our comparison, we present the accuracy values of the preprocessed input text and the baseline tokenized one. 

\begin{table}
\centering
\caption{Accuracy of the input text and the baseline segmented one}
\label{tab:base}
\begin{tabular}{ l  r } 
\hline
& Accuracy \\ 
\hline
% Original & \% \\
Preprocessed  & 97.55\% \\
Segmented (baseline) & 99.11\% \\
\hline
\end{tabular}
\end{table}

Accuracy values in Table \ref{tab:base} can be used as good bases for the comparison of the overall segmentation task, but one must note that this metric is not well balanced. 
%This is a special property of the applied metric, 
Its values are relatively high even for the preprocessed text, thus the improvement in accuracy is measured. 
Relative error rate reduction scores are provided in Table \ref{tab:reduction}, which are calculated over the baseline method. We investigated each part of the sentence segmentation algorithm and examined their collaboration as well. The unsupervised SBD algorithm is marked with \emph{LLR}\footnote{Referring to the term log-likelihood ratio.}, while the second component is indicated by the term \emph{CAP}.

\begin{table}
\centering
\caption{Error rate reduction over the accuracy of the baseline method}
\label{tab:reduction}
\begin{tabular}{ l  r } 
\hline
& Error rate reduction\\
\hline
LLR & 58.62\% \\
CAP & 9.25\% \\
LLR+CAP & 65.50\% \\
\hline
\end{tabular}
\end{table}


A more detailed analysis of the SBD task is made by comparing precision, recall and $F_{0.5}$ values in Table \ref{tab:prec_rec}. These results show that each component significantly increases the recall, while precision is just barely decreased. All in all, the combined hybrid algorithm\footnote{It is the composition of the BWT, BSBD, LLR and CAP components.} brings significant improvement over the well-established baseline.

\begin{table}
\centering
\caption{Evaluation of the proposed sentence segmentation algorithm compared with the baseline}
\label{tab:prec_rec}
\begin{tabular}{ l r @{\hspace{0.3cm}} r @{\hspace{0.3cm}} r  } 
\hline
& Precision & Recall & $F_{0.5}$ \\
\hline
Baseline & 96.57\% & 50.26\% & 81.54\%  \\
LLR & 95.19\% & 78.19\% & 91.22\% \\
CAP & 94.60\% & 71.56\% & 88.88\% \\
LLR+CAP & 93.28\% & 86.73\% & \underline{91.89\%} \\
\hline
\end{tabular}
\end{table}



Moving on, we compare our method with other available methods as well. 
Freely available tools for segmenting Hungarian texts are magyarlanc and Huntoken. 
The latter can be slightly configured by providing a set of abbreviations, thus two versions of Huntoken are used. 
The first employs a set of general Hungarian abbreviations (\emph{HTG}), the second utilizes an extended dictionary\footnote{As described in section \ref{sec:abbrev}.} containing medical ones as well(\emph{HTM}). 
%Since we are following the unsupervised approach of Kiss and Strunk, their system 
Punkt \cite{kiss2006unsupervised} and OpenNLP \cite{Baldridge2002} -- a popular implementation of the maximum entropy SBD method \cite{reynar1997maximum} -- were involved in the comparison as well. The latter tool employs supervised learning, thus the Hungarian Szeged Corpus is used as a training material. 

\begin{table}
\centering
\caption{Comparision of the proposed hybrid SBD method with competing ones}
\label{tab:comparison}
\begin{tabular}{ l r @{\hspace{0.3cm}} r @{\hspace{0.3cm}} r} 
\hline
& Precision & Recall & $F_{0.5}$ \\
\hline
magyarlanc & 72.59\% & 77.68\% & 73.55\% \\
HTG & 44.73\% & 49.23\% & 45.56\% \\
HTM & 43.19\% & 42.09\% & 42.97\% \\
Punkt & 58.78\% & 45.66\% & 55.59\%  \\
OpenNLP & 52.10\% & 96.30\% & 57.37\% \\
Hybrid system & 93.28\% & 86.73\% & \underline{91.89\%} \\
\hline
\end{tabular}
\end{table}

Values in Table \ref{tab:comparison} show that general segmentation methods failed on Hungarian clinical records. 
Results also show that the maxent approach has high recall, but boundaries marked by it are false positives in almost half of the cases. 
Rules provided by magyarlanc seem to be robust, but the overall performance inhibits its application for clinical texts. 
Others do not just provide low recalls, but their precision values are around 50\% being too low for practical purposes. 

Our approach mainly focuses on the sentence segmentation task, but an improvement of word tokenization is expected as well. Better recognition of words %that are not abbreviations 
results in a higher recall (see Table \ref{tab:tok_eval}), while it does not significantly decrease precision. \label{sec:eval}

\begin{table}
\centering
\caption{Comparing tokenization performance of the new tool with the baseline one}
\label{tab:tok_eval}
\begin{tabular}{ l r @{\hspace{0.3cm}} r @{\hspace{0.3cm}}  r} 
\hline
& Precision & Recall & $F_{1}$ \\
\hline
Baseline & 99.74\% & 74.94\% & 85.58\%  \\
Hybrid system & 98.54\% & 95.32\% & \underline{96.90\%} \\
\hline
\end{tabular}
\end{table}


All in all, the segmentation method successfully deals with several sorts of imperfect sentence boundaries.
As described in section \ref{sec:eval}, the given algorithm performs better in terms of precision and recall than competing ones. 
Only \texttt{magyarlanc} reached an $F_{0.5}$-score above 60\%, which is still too low for practical applications. 
The method presented in this study achieved a 92\% of $F_{0.5}$-score allowing its utilization for clinical Hungarian.

%TODO: ITT

\section{Tagging Clinical Notes}

While tagging of general texts is well-known and considered to be solved, most of the commonly used methods fail on medical texts. Further on, English has been the main target of many NLP applications up to the present time, thus less-resourced languages, which are usually morphologically complex, often fell beyond the scope. Similarly, there are just a few studies attempting to annotate non-English medical texts. Further on, the processing of Hungarian clinical records has very little literature not containing any attempts on morphological tagging. This study investigates how existing techniques can be used for the morphological tagging of Hungarian clinical records.

This section is structured as follows. The background of our research is described first. Then a corpus is presented which has been created for development and evaluation purposes. In Section \ref{sec:baseline}, we detail a baseline morphological disambiguation chain based on PurePos. Afterwards, we present the most frequent errors made by the baseline tagger and we describe and evaluate the enhancements that were carried out on the text processing chain.  

\subsection{Background}

\subsubsection{Biomedical tagging}\label{sec:biomed_tag}

Processing of biomedical texts has an extensive literature, since there are numerous resources accessible. In contrast, much less manually annotated corpora of clinical texts are available. Most of the work in this field has been done for English, and  only a few attempts have been published for morphologically rich languages (e. g. \cite{oleynik2009performance,rost2008lessons}).

A general approach for biomedical PoS tagging is to employ supervised learning algorithms, which require manually annotated data. 
%However, these methods require manually disambiguated corpora. 
In the case of tagging biomedical texts, domain-specific corpora are used either alone \cite{pakhomov2006developing,savova2010mayo,Smith2006} or in conjunction with a (sub)corpus of general English \cite{coden2005domain,ferraro2013improving,miller2007building} as training data. While using texts only from the target domain yields acceptable performance \cite{pakhomov2006developing,savova2010mayo,Smith2006}, 
several experiments have shown that accuracy further increases with incorporating annotated sentences from the general domain as well \cite{barrett2011token,coden2005domain}. A general observation is that the more data is used from the reference domain, the higher accuracy can be achieved (e. g. \cite{pestian2004development}). On the contrary, Hahn and Wermter argue for training learners only on general corpora \cite{hahn2004tagging} (for German). Further on, there are studies on selecting training data (e. g. \cite{liu2007heuristic}) that increase the accuracy. What is more, there are taggers (such as \cite{choi2012fast}) which learn from several domains in a parallel fashion, thus the model selection decision is delayed until the decoding process. 

Using target-specific lexicons is another way of adapting taggers, as they can improve tagging performance \cite{coden2005domain,ruch2000minimal}.  Some of these studies extend existing PoS dictionaries \cite{divita2006dtagger}, while others build new ones specific to the target domain \cite{Smith2006}. All of the experiments using such resources yield significantly reduced error rates. 

Concerning tagging algorithms, researchers tend to prefer already existing applications, such as the OpenNLP toolkit\footnote{\url{http://opennlp.apache.org/}}, which is the basis of the cTakes system \cite{savova2010mayo}; 
while %TreeTagger \cite{Schmid1994}, 
Brill’s method \cite{Brill1992} and TnT \cite{Brants2000} are widely used (e.g. \cite{hahn2004tagging,savova2010mayo,pestian2004development}) as well. There are other HMM-based solutions which have been shown to perform well \cite{barrett2011token,coden2005domain,divita2006dtagger,hahn2004tagging,pakhomov2006developing,rost2008lessons,ruch2000minimal} on such texts. Besides, a number of experiments have revealed \cite{ferraro2013improving,ruch2000minimal,Smith2006} that domain-specific OOV words are primarily responsible for a reduced tagging performance. Thus successful methods employ either guessing algorithms \cite{barrett2011token,divita2006dtagger,rost2008lessons,ruch2000minimal,Smith2006} or broad-coverage lexicons (as detailed above). Beyond supervised algorithms, other approaches were also shown to be effective: Miller et al. \cite{miller2007building} use semi-supervised methods;
%\footnote{This algorithm needs raw data from the target domain, while an annotated general corpus is still used.}; 
Dwinedi and Sukhadeve build a tagger system based only on rules \cite{dwivedi8rule}; while Ruch et al. propose a hybrid system \cite{ruch2000minimal}. Further on, domain adaptation methods (such as EasyAdapt \cite{daume2007frustratingly} or ClinAdapt \cite{ferraro2013improving} 
%or reference distribution modelling  \cite{tateisi2006subdomain}
) also perform well. However, they need an appropriate amount of manually annotated data from the target domain, which limits their applicability. 

First we examine special properties of clinical notes first, then a proper disambiguation methodology. The experiments described below use methods that rely on an error analysis of the baseline system (in Section \ref{sec:baseline}), while also incorporate ideas from previous studies (cf. Section  \ref{sec:biomed_tag}).

\subsection{The clinical corpus}

First of all, special properties of clinical texts need to be considered. Such records are created in a special environment, thus they differ from general Hungarian in several respects. These attributes are the following (cf. \cite{Orosz2013a,Siklosi2013b,Siklosi2012}):
\begin{inparaenum}[\itshape a\upshape)]
 \item notes contain a lot of erroneously spelled words,
 \item sentences generally lack punctuation marks and sentence initial capitalization, 
 \item measurements are frequent and have plenty of different (erroneous) forms,
 \item a lot of (non-standard) abbreviations occur in such texts, 
 \item and numerous medical terms are used that originate from Latin.
\end{inparaenum}

Since there was no corpus of clinical records available that was manually annotated with morphological analyses, a new one was created for testing purposes. This corpus contains about 600 sentences, which were extracted from the notes of 24 different clinics. First, the textual parts of the records were identified (as described in \cite{Siklosi2012}), then the paragraphs to be processed were selected randomly. Then manual sentence boundary segmentation, tokenization and normalization was performed, which were aided by methods detailed in \cite{Orosz2013a}. Manual spelling correction was carried out by using suggestions provided by the system of Siklósi et al. \cite{Siklosi2013}. Finally, morphological disambiguation was performed: the initial annotation was provided by PurePos, then its output was checked manually. 

%Similarly to clinical texts, 
Several properties of the corpus created differ from general ones. Beside characteristics described above, the corpus contains numerous \textit{x} tokens which denote multiplication and are labeled as numerals. Latin words and abbreviations are analyzed regarding their meaning: e.g. \textit{o.} denotes \textit{szem} `eye’, thus it is tagged with \textsc{n.nom}. Further on, names of medicines are labeled as singular nouns. Finally, as missing sentence final punctuation marks were not recovered in the test corpus, these are not tagged either. 

\begin{table}
\centering
\caption{Size of the clinical corpus created}
\label{tab:clin_corpus}
\begin{tabular}{ l @{\hspace{0.3cm}} r @{\hspace{0.3cm}} r } 
\hline
& Sentences & Tokens \\
\hline
Development set & 240 & 2230 \\
Test set & 333 & 3155 \\
\hline
\end{tabular}
\end{table}

The corpus was split into a development and a test set (see Table \ref{tab:clin_corpus}). The first part was employed for development purposes, while the methods detailed below were evaluated against the second part. Evaluation was carried out by calculating per-word accuracy omitting punctuation marks. 

\subsection{The baseline settings}

\label{sec:baseline}

Below we introduce the baseline tagging chain. First we describe its components, then the performance of the tagger is evaluated by detailing the most common error types. Concerning the parts of the chain we follow the work of Orosz et al. \cite{Orosz2013b}.  Thus \emph{(morphosyntactic tag, lemma)} pairs represent the analyses of HuMor, which are then disambiguated by PurePos. However, the output of the MA is extended with the new analyses of \textit{x} in order to fit the corpus to be tagged. 

This baseline text processing chain produced 86.61\% token accuracy on the development set, which is remarkably lower than tagging results for general Hungarian using the same components (96--98\% \cite{Orosz2012}). Measuring the ratio of the correctly tagged sentences revealed that less than the third (28.33\%) of the sentences were tagged correctly. This amount indicates that the models used by the baseline algorithm are weak for such a task. Therefore, errors made by the baseline algorithm are investigated first to reveal how the performance could be improved. 

\begin{table}
\centering
\caption{Distribution of errors caused by the baseline algorithm -- dev. set}
\label{tab:error_types}
\begin{tabular}{ l . } 
\hline
Class & \multicolumn{1}{c}{Frequency}  \\
\hline
Abbreviations and acronyms & 49.17\% \\
Out-of-vocabulary words & 27.27\% \\
Domain-specific PoS of word forms & 14.88\% \\
% Numbers & 0.02\% \\
Other & 0.06\% \\
\hline
\end{tabular}
\end{table}

Table \ref{tab:error_types} shows that the top error class is the mistagged abbreviations and acronyms. 
%These mistakes are mainly due to the complex tagging scheme of the corpus, since abbreviated terms are annotated regarding their meaning. 
A reason for the high number of such errors is that most of these tokens are unknown to the tagger. Moreover, abbreviations usually refer to medical terms that originate from Latin.

Another frequent error type is caused by the out-of-vocabulary (OOV) words. This observation is in accordance with the PoS tagging results for medical English (as described above). Similarly, in the case of Hungarian, most of the OOV tokens are specific to the clinical domain and often originate from Latin. However, several inflected forms of such terms also exist in clinical notes due to agglutination. Therefore, listing only medical terms and their analyses could not be a proper solution. This problem requires complex algorithms.

Furthermore, the domain-specific usage of general words leads the tagger astray as well. Frequently, participles are labeled as verbs such as \textit{javasolt} `suggested’ or  \textit{felírt} `written’. In addition, numerous mistakes are due to the lexical ambiguity that is present in Hungarian 
%. Examples are: \textit{lép} that means either `spleen’ (organ) or 
(such as \textit{szembe} which can refer to `into an eye’ or `toward/against’). 

Our investigation shows that most of the errors of the baseline system can be classified into the three categories above. 
%Apart from the low accuracy of the tagger, 
We can use the categorization above to enhance the performance of the system by eliminating the typical sources of errors.

\subsection{Domain adaptation experiments}

Based on the observations above, systematic changes were carried out to improve the tagging accuracy of the chain. First, the processes of lexicon extension and algorithmic modifications are described, then an investigation is presented aiming to find the optimal training data. Each enhancement is evaluated against the test corpus. Table \ref{tab:improvements} contains the part-of-speech tagging, lemmatization and the whole morphological tagging performance of each system.

\begin{table}
\centering
\caption{Evaluation of the enhancements -- test set}
\label{tab:improvements}
\begin{tabular}{ l l . . .} 
\hline
ID & Method & \multicolumn{1}{r}{PoS tagging\ \ } & \multicolumn{1}{r}{Lemmatization\ \ } & \multicolumn{1}{r}{Morph. disambig.} \\
\hline
0 & Baseline system & 90.57\% & 93.54\% & 88.09\% \\
1 & 0 + Lexicon extension & 93.89\% & 96.24\% & 92.41\% \\
2 & 1 + Handling abbreviations & \multicolumn{1}{Z{.}{.}{-1}}{94.81\%} & \multicolumn{1}{Z{.}{.}{-1}}{97.60\%} & \multicolumn{1}{Z{.}{.}{-1}}{93.73\%} \\
3 & 2 + Training data selection & 94.25\% & 97.36\% & 93.29\% \\
\hline
\end{tabular}
\end{table}

\subsubsection{Extending the lexicon of the morphological analyzer}
\label{sec:ma-extension}
Supervised tagging algorithms commonly use augmented lexicons in order to reduce the number of out-of-vocabulary words (see Section \ref{sec:biomed_tag}). In the case of Hungarian, this must be performed at the level of the MA. Here we describe the process which was carried out to extend the lexicon of the HuMor analyzer.

The primary source for the extension process was a spelling dictionary of medical terms \cite{Fabian1992} that contained about 90000 entries. Beside this, a freely available list of medicines \cite{Foigazgatosag2012} of about 38000 items was used as well. Since neither of these resources contained any morphological information concerning these words, such analyses were created. For this, we followed an iterative process which included both human work and automatic algorithms. The steps of our workflow were the following: 1) a set of word forms was prepared and analyzed automatically (detailed below); 2) the analyses were checked and corrected manually; 3) the training sets of the supervised learning methods were extended with the results of step 2). Before each iteration, compounds of known items were selected to be processed first. This enhancement reduced the time spent on manual correction and granted the consistency of the database created. In the end, approximately 41000 new entries were added to the lexicon of the HuMor analyzer.

Since Latinate words can either be written as pronounced in Hungarian\footnote{An example is the Latin word \emph{dysplasia} \textipa{[displa\*:zia]}, which can be spelled as \emph{diszplázia} in Hungarian.} or can appear with the original Latin spelling, having both variants is necessary. Most of the entries in the dictionary had both the Hungarian and Latin spelling variants, but this was not always the case. Language identification of the words was carried out to distinguish Hungarian terms from the ones that have Greek, Latin, English or French spelling. For this, an adapted version of TextCat \cite{Cavnar1994} was involved in the iterative process to decide whether a word is Hungarian or not. If it was necessary, missing Hungarian spelling variants were produced using letter-to-sound rules of Latinate words as they are generally pronounced in Hungarian and were added semi-automatically to the lexicon.

As for the calculation of the morphological analyses, the guesser algorithm of PurePos was employed. Separate modules were employed for each language, thus language-specific training sets were maintained for them as well. 
%Beside the analyses, inflection paradigms of non-Hungarian words had to be determined as well. 
In Hungarian, the inflection paradigm depends on vowel harmony and the ending of the word as it is pronounced, thus the pronunciation of foreign words had to be calculated first. This could be carried out using the same simple hand-written rules implementing Latin grapheme-to-phoneme correspondences that were used to generate missing Hungarian spelling variants.
%, since Hungarian words are written as they are pronounced.

The lexicon extension process above reduced the OOV word ratio from 34.57\% to 26.19\% (development set), and resulted in an accuracy of 92.41\% (test set). Since the medical dictionary \cite{Fabian1992} contained abbreviated words as well, this process could also decrease the number of mistagged abbreviations.

\subsubsection{Dealing with acronyms and abbreviations}

Despite the changes in Section \ref{sec:ma-extension}, numerous errors made by the enhanced tagger were still connected to abbreviations. Thus %finding proper methods for handling abbreviations is a crucial part of such a parsing algorithm. For this, 
we first examined erroneous tags of abbreviated terms, then developed methods aiming to improve the performance of the disambiguation chain. %Finally, experiments’ results are presented in Table \ref{tab:abbrev_fixes}.

A detailed error analysis revealed that some of the erroneous tags of abbreviated terms were due to the over-generating nature of HuMor, which could be reduced by a filtering method. 
%Thus the first enhancement performed was to leave out invalid analysis candidates: 
For words with full stops an analysis was considered to be false if its lemma was not an abbreviation. This modification increased the overall accuracy significantly, reducing the number of errors by 9.20\% on the development set (cf. ``Filtering'' in Table \ref{tab:abbrev_fixes}).

Another typical error type was the erroneous tagging of unknown acronyms. Since PurePos did not employ features that could deal with such cases, these tokens were left to the guesser. However, acronyms should have been tagged as singular nouns. Thus a pattern matching component relying on surface features could fix their tagging (see ``Acronyms'' in Table \ref{tab:abbrev_fixes}). 

The rest of the errors were mainly connected to those abbreviations that were both unknown to the analyzer and had not been seen previously. For this, the distribution of the labels of abbreviations in the development data is compared to that of the Szeged Corpus (see Table \ref{tab:pos_distribution} below).
While there are several common properties between the two columns (such as the ratio of adverbs), discrepancies occur even more often. One of them is the ratio of adjectives, which is significantly higher in the medical domain than in general Hungarian. Comparing the values, it must be noted that 10.85\% of the tokens are abbreviated in the development set, while the same ratio is only 0.37\% in the Szeged Corpus. 

\begin{table}
\centering
\caption{Morphosyntactic tag frequencies of abbreviations -- dev. set}
\label{tab:pos_distribution}
\begin{tabular}{ l . .} 
\hline
Tag & \multicolumn{1}{c}{\ \ Clinical texts} & \multicolumn{1}{c}{\ \ Szeged Corpus}  \\ 
\hline
\scshape{n.nom} & 67.37\% & 78.18\% \\
\scshape{a.nom} & 19.07\% & 3.96\% \\
\scshape{conj} & 1.27\% & 0.50\% \\
\scshape{adv} & 10.17\% & 11.86\% \\
% \scshape{x} & 0.00\% & 5.42\% \\
Other & 2.12\% & 5.50\% \\
\hline
\end{tabular}
\end{table}

Since the noun tag was the most frequent amongst abbreviations, a plausible method  was to assign \textsc{n.nom} to all of these tokens (cf.  ``UnkN'' in Table \ref{tab:abbrev_fixes}) and to keep the original word forms as lemmata. This baseline method resulted in a surprisingly high error rate reduction of 31.54\%. 

Another approach was to model the analyses of abbreviations with data observed in Table \ref{tab:pos_distribution}. The first experiment (``UnkUni'') employed a uniform distribution of labels for abbreviations present in the development set as an emission probability distribution. 
Thus all the tags (\textsc{a.nom}, \textsc{a.pro}, \textsc{adv}, \textsc{conj}, \textsc{n.nom}, \textsc{v.3sg}, \textsc{v.pst\_ptcl}) were used with equal probability as a sort of guessing algorithm.

Beside this, a better method was to use a maximum likelihood estimation for calculating a priori probabilities (``UnkMLE''). In this case,  relative frequency estimates were calculated for all the tags above.
%Evaluation of the methodologies showed (cf. Table \ref{tab:abbrev_fixes}) that 
While the latter approaches could increase the overall performance, none of them managed to reach the accuracy of the ``UnkN'' method (cf. Table \ref{tab:abbrev_fixes}). 

\begin{table}
\centering
\caption{Comparison of the approaches aiming to handle acronyms and abbreviations --  dev. set}
\label{tab:abbrev_fixes}
\begin{tabular}{ l l . } 
\hline
ID & Method &  \multicolumn{1}{c}{Morph. disambig.} \\
\hline
0 & Medical lexicon & 90.11\% \\
1 & 0 + Filtering & 91.02\% \\
2 & 1 + Acronyms & 91.41\% \\
3 & 2 + UnkN & \multicolumn{1}{Z{.}{.}{-1}}{94.12\%} \\
4 & 2 + UnkUni & 92.82\% \\
5 & 2 + UnkMLE & 94.01\% \\
\hline
\end{tabular}
\end{table}

\subsubsection{Choosing the proper training data}

Since many studies showed (cf. Section \ref{sec:biomed_tag}) that the training data used significantly affects the result of the annotation chain, we investigated the usage of sub-corpora available in the Szeged Corpus. Several properties of the corpus were examined (cf. Table \ref{tab:subcorpora_attrib}) in order to find the training dataset that fits best for tagging clinical Hungarian. Measurements regarding the development set were calculated manually where it was necessary.

% 2,318.02
% 995.57
% 1,335.90
% 854.11
% 1,284.89
% 824.42
% 859.33

\begin{table}
\centering
\caption{Properties of training corpora}
\label{tab:subcorpora_attrib}
\begin{tabular}{ l . . . . . } 
\hline
\multicolumn{1}{l}{\multirow{2}{*}{Corpus}} & \multicolumn{1}{c}{Avg. sent.} & \multicolumn{1}{c}{Abbrev.}  &  \multicolumn{1}{c}{\ \ Unknown\ \ } & \multicolumn{2}{c}{\ \ \ \ Perplexity}\\
 & \multicolumn{1}{c}{length} & \multicolumn{1}{c}{ratio} &  \multicolumn{1}{c}{ratio} & \multicolumn{1}{c}{Words} & \multicolumn{1}{c}{Tags} \\
\hline
Szeged Corpus & 16.82 & 0.37\%\ \ \  & \multicolumn{1}{Z{.}{.}{-1}}{1.78\%} & \ \ 2318.02 & 22.56\\
\hspace{0.2cm} Fiction & 12.30 & 0.10\% & 2.44\% & 995.57 & 32.57\\
\hspace{0.2cm} Compositions & 13.22 & 0.14\% & 2.29\% & 1335.90 & 30.78\\
\hspace{0.2cm} Computer & 20.75 & 0.14\% & 2.34\% & 854.11 & 22.89\\
\hspace{0.2cm} Newspaper & 21.05 & 0.20\% & 2.10\% & 1284.89 & \multicolumn{1}{Z{.}{.}{-1}}{22.08}\\
\hspace{0.2cm} Law & 23.64 & 1.43\% & 2.74\% & \multicolumn{1}{Z{.}{.}{-1}}{824.42} & 29.79\\
\hspace{0.2cm} Short business news & 23.28 & 0.91\% & 2.50\% & 859.33 & 27.88\\
% \hline
Development set & 9.29 & 10.85\% & \multicolumn{1}{c}{\ \ --} & \multicolumn{1}{c}{\ --} & \multicolumn{1}{c}{\ --}\\
\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

First of all, an important attribute of a corpus is the length of its sentences. Texts having shorter sentences tend to have simpler grammatical structure, while longer sentences are grammatically more complex. Further on, clinical texts have a vast amount of abbreviations, thus the ratio of abbreviations is also relevant during the comparison. 

Furthermore, the accuracy of a tagging system is strongly related to the ratio of unknown words, thus these proportions were calculated for the development set using the vocabulary of each training corpus (see Table \ref{tab:subcorpora_attrib}). This ratio could function as a similarity metric, but entropy-based measures work better \cite{kilgarriff1998measures} in such scenarios. We use perplexity, which is calculated here as follows: trigram models of word and tag sequences are trained on each corpus using Kneser-Ney smoothing, then all of them are evaluated against the development set\footnote{The SRILM toolkit \cite{stolcke2002srilm} was employed for the calculations.}.

Measurements show that there is no such part of the Szeged Corpus which has as much abbreviated terms as clinical texts have. Likewise, sentences written by clinicians are significantly shorter than the ones in any of the genres present in the Szeged Corpus. Neither the calculations above, nor the ratio of unknown words suggest that we should use subcorpora for training. However, the perplexity scores contradict this: sentences from the law domain have the most phrases in common with clinical notes, while news texts have the most similar grammatical structures. 

Therefore, all sub-corpora were involved in the evaluation, which was carried out by employing all of the enhancements described in previous sections. Results showed that training on news texts resulted in the highest accuracy. However, it was not able to outperform the usage of the whole corpus.


\begin{table}
\centering
\caption{Evaluation of the tagger using the subcorpora as training data -- test set}
\label{tab:eval_subcorpora}
\begin{tabular}{ l . } 
\hline
Corpus & \multicolumn{1}{c}{Morph. disambiguation accuracy} \\
\hline
Szeged Corpus & \multicolumn{1}{Z{.}{.}{-1}}{93.73\%} \\
\hspace{0.2cm} Fiction & 92.01\% \\
\hspace{0.2cm} Compositions & 91.97\% \\
\hspace{0.2cm} Computer & 92.73\% \\
\hspace{0.2cm} Newspaper & \multicolumn{1}{Z{.}{.}{-1}}{93.29\%} \\
\hspace{0.2cm} Law & 92.17\% \\
\hspace{0.2cm} Short business news & 92.69\% \\
\hline
\end{tabular}
\end{table}


\subsection{Evaluation}

%TODO az elejéről ide kellene hozni a táblázatot + néhány sor + konklúzió

In this study, resources and methodologies were introduced that enabled us to investigate morphological tagging of clinical Hungarian. First, a test corpus was created and was compared in detail with a general Hungarian corpus. This corpus also allowed for the evaluation of numerous tagging approaches. These experiments were based on the PurePos tagger tool and the HuMor morphological analyzer. Errors made by the baseline morphological disambiguation chain were investigated, then several enhancements were carried out aiming at correcting the most common mistakes of the baseline algorithm. Amongst others, we extended the lexicon of the morphological analyzer and introduced several methods to handle the errors caused by abbreviations. 

The baseline setup labeled every eighth token erroneously. Although this tagging chain is commonly used for parsing general Hungarian, it resulted in mistagged medical sentences in two thirds of the cases. In contrast, our enhancements raised the ceiling of the tagging accuracy to 93.73\% by eliminating almost half (47.36\%) of the mistakes. Deeper investigation revealed that this error rate reduction was mainly due to the usage of the extended lexicon, which significantly decreased the number of the out-of-vocabulary tokens. While this research did not manage to find decent training data for tagging clinical Hungarian, it showed that neither part of the Szeged Corpus was able to outperform the whole as a training corpus. Finally, results of tagging abbreviations suggest that abbreviated terms should not be tagged directly. They should be resolved first or should be labeled with a uniform tag.% The latter approach would let further practical applications to handle them. 

The main limitation of this research is the corpus used. It contains a few hundred sentences, which is only enough to reveal the main pitfalls of the tagging method. Furthermore, most of the domain adaptation methods rely on target-specific corpora that have several thousands of sentences. Taking these into consideration, further investigation should involve more manually annotated data from the medical domain. 

In sum, commonly used methodologies alone fail to tag Hungarian clinical texts with a satisfactory accuracy. One of the main problems is that such algorithms are not able to deal with the tagging of abbreviations. However, our results suggests that the usage of an extended lexicon considerably increases the accuracy of an HMM tagger. 



